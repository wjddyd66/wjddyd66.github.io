---
layout: post
title:  "Ch4.Training Linear Model"
date:   2020-07-06 09:15:20 +0700
categories: [Handson]
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

## Training Lienar Model
ì´ì „ Postì—ì„œëŠ” ë‹¨ìˆœí•˜ê²Œ Sklearnì—ì„œ Modelë“¤ì„ ê°€ì§€ê³  Fittingì„ í•˜ì˜€ë‹¤.  
ì´ë²ˆ Postì—ì„œëŠ” ë‹¨ìˆœí•˜ê²Œ Modelë§Œ ê°€ì§€ê³  Fittingí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²•ë° ì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ Parameterì¡°ì ˆ, ë˜ Modelì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³¸ë‹¤.

## Setup
ì‹¤ì œ Projectë¥¼ ì§„í–‰í•˜ê¸° ì•ì„œ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” Libraryí™•ì¸ ë° ì›í•˜ëŠ” Version(Python ì–¸ì–´ íŠ¹ì„±ìƒ Versionì— ë§ì´ ì˜ì¡´í•˜ê²Œ ëœë‹¤.)ì´ ì„¤ì¹˜ë˜ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì‘ì—…ì´ë‹¤.  
ë˜í•œ, ìì£¼ ì‚¬ìš©í•˜ê²Œ ë  Functionì´ë‚˜, Directoryë¥¼ ì§€ì •í•˜ê¸°ë„ í•œë‹¤.


```python
# Python â‰¥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn â‰¥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(42)

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training_linear_models"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
```

## Linear Regression
ì„ í˜• íšŒê·€ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” <a href="https://wjddyd66.github.io/dl/NeuralNetwork-(3)-Optimazation/">NormalEquationê³¼ SGDì™€ ê°™ì´ Optimizerë¥¼ í™œìš©í•˜ì—¬ Function Approximationì„ í•˜ëŠ” ë°©ë²•</a>ì´ ìˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ ì´ë²ˆ Postë¥¼ ë²—ì–´ë‚˜ë¯€ë¡œ ë§í¬ ì°¸ì¡°)  
ì•„ë˜ CodeëŠ” <span>$$y=4+3*X+GausianNoise$$</span>ì˜ Dataë¥¼ ìƒì„±í•˜ê³  í™•ì¸í•˜ëŠ” ê³¼ì •ì´ë‹¤.
- <code>np.random.rand(m,n)</code>: m,n í¬ê¸°ì˜ 0~1 í‘œì¤€ì •ê·œ ë¶„í¬
- <code>np.random.randn(100,1)</code>: m,n í¬ê¸°ì˜ í‰ê· 0, ë¶„ì‚°1ì˜ ê°€ìš°ì‹œì•ˆ í‘œì¤€ì •ê·œ ë¶„í¬


```python
# Create Data
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Data Visualization
plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
save_fig("generated_data_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_3_1.png)


### Normal Equation

ìœ„ì—ì„œ ìš°ë¦¬ëŠ” ì‹ <span>$$y=4+3*X+GausianNoise$$</span>ìœ¼ë¡œì„œ Datasetì„ êµ¬ì¶•í•˜ì˜€ë‹¤.  

ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì‹¤ì œ Functionì„ ì•Œ ìˆ˜ ì—†ê³  ì£¼ì–´ì§„ X,Yì˜ Dataë¡œì„œ ìœ„ì˜ Functionì„ ì˜ˆì¸¡í•˜ì—¬ì•¼ í•œë‹¤.  
ë¨¼ì € ìš°ë¦¬ëŠ” **Modelì´ Linear Regressioní˜•íƒœë¼ê³  ê°€ì •í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‹ì„ ì“¸ ìˆ˜ ìˆë‹¤.**  
<p>$$y = \theta_0+\theta_1*x$$</p>

ìœ„ì˜ ì‹ì—ì„œ ìš°ë¦¬ëŠ” **Normal Equationì„ ì‚¬ìš©**í•˜ê²Œ ë˜ë©´ ìµœì ì˜ Parameter <span>$$\hat{\theta} = (X^{T}X)^{-1}X^{T}y$$</span>ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ìœ„ì˜ ì‹ì´ ì´í•´ ë˜ì§€ ì•Šìœ¼ì‹œë©´ ìœ„ì˜ ë§í¬ë¥¼ ì°¸ì¡°)  

ë”°ë¼ì„œ <span>$$\theta_0 = \theta_0*X_0 -> X_0=1$$</span>ì´ ë˜ê²Œ ë˜ê³  ì´ëŸ¬í•œ ìƒíƒœì—ì„œ NormalEquationì„ í†µí•´ <span>$$\hat{\theta}$$</span>ë¥¼ êµ¬í•˜ì—¬ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.


```python
# X_0 + X_1
X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance
# Normal Equation
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
print('Paramter\nğœƒ0 = {}, ğœƒ1 = {}'.format(theta_best[0],theta_best[1]))

# Prediction
X_new = np.array([[0],[2]])
X_new_b = np.c_[np.ones((2,1)),X_new]
# X =0, 2 -> y=?, 
y_pred = X_new_b.dot(theta_best)
print('\nModel Prediction\n',y_pred,'\n')

# Visualization
save_fig("linear_model_predictions_plot")
plt.plot(X_new, y_pred, "r-")
plt.plot(X,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```

    Paramter
    ğœƒ0 = [4.21509616], ğœƒ1 = [2.77011339]
    
    Model Prediction
     [[4.21509616]
     [9.75532293]] 


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_5_1.png)


ìœ„ì˜ ê³¼ì •ì€ sklearnì˜ Modelì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.


```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X,y)

print('Paramter\nğœƒ0 = {}, ğœƒ1 = {}'.format(lin_reg.intercept_, lin_reg.coef_))
```

    Paramter
    ğœƒ0 = [4.21509616], ğœƒ1 = [[2.77011339]]


ìœ„ì˜ ì‹ì„ ì¡°ê¸ˆë§Œ ë” ìƒê°í•´ë³´ì.  
ìœ„ì—ì„œ Normal Equationì˜ ê²°ê³¼ë¡œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì—ˆë‹¤.  
<p>$$\hat{\theta} = (X^{T}X)^{-1}X^{T}y$$</p>
ìœ„ì˜ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í•  ìˆ˜ ìˆë‹¤.  
<p>$$(X^{T}X)^{-1}X^{T}y = X^{-1}(X^{T})^{-1}X^{T}y$$</p>
<p>$$X^{-1}y = X^{+}y$$</p>
<p>$$(X^{+}\text{  :pseudo inverse})$$</p>


```python
np.linalg.pinv(X_b).dot(y)
```




    array([[4.21509616],
           [2.77011339]])



<a href="https://wjddyd66.github.io/dl/NeuralNetwork-(3)-Optimazation/">Optimization</a>ì„ ì‚´í´ë³´ê²Œ ë˜ë©´, í¬ê²Œ 2ê°€ì§€ì˜ ë‹¨ì ì´ ì¡´ì¬í•˜ê²Œ ëœë‹¤.
1. Dataì˜ ëª¨ë“  Inputì„ ì•Œì•„ì•¼ í•œë‹¤.
2. Dimensionì´ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ê³„ì‚°ì„ ìœ„í•œ ì‹œê°„ ë° ë©”ëª¨ë¦¬ê°€ ë§ì´ ì†Œìš©ëœë‹¤.

ìœ„ì™€ ê°™ì€ NormalEquaitonì˜ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•œ optimizationì˜ ë°©ë²•ì€ Gradient Descentë°©ë²•ì´ë‹¤.

## Linear regression using batch gradient decent

Gradient DescentëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
- Model: <span>$$\hat{y^{(i)}} = \theta^T x^{(i)}$$</span>
- LossFunction(Mean Square Error): <span>$$\sum_{i=1}^{m}(y^{(i)}-y^{(i)})^2$$</span>
- Optimization(Batch Gradient Descent): <span>$$\frac{\partial}{\partial \theta_j}MSE(\theta) = \frac{2}{m}\sum_{i=1}^{m}(\theta^T x^{(i)} - y^{(i)})x_j^{(i)}$$</span>
- Weight Update: <span>$$\theta^{*} = \theta -\eta \nabla_{\theta}MSE(\theta) \text{,   }\eta:\text{  Learning Rate}$$</span>


```python
eta = 0.1  # learning rate
n_iterations = 1000
m = 100

theta = np.random.randn(2,1)  # random initialization

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
    
hat_y = X_new_b.dot(theta)

print('Paramter: ğœƒ0 = {}, ğœƒ1 = {}'.format(theta[0], theta[1]))
print('Model Prediction: y1 = {} ğ‘¦Ì‚1 = {} , y2 = {} ğ‘¦Ì‚2 = {}'.format(4,hat_y[0],10,hat_y[1]))
```

    Paramter: ğœƒ0 = [4.21509616], ğœƒ1 = [2.77011339]
    Model Prediction: y1 = 4 ğ‘¦Ì‚1 = [4.21509616] , y2 = 10 ğ‘¦Ì‚2 = [9.75532293]


Weight(<span>$$\theta$$</span>)ì˜ Updateê°€ ì˜ì´ë£¨ì›Œì§€ëŠ” ê²ƒ ì•Œ ìˆ˜ ìˆë‹¤.  

**Batch Gradient DescentëŠ” ë§¤ Step(iteration)ì—ì„œ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ Xì— ëŒ€í•´ ê³„ì‚°í•œë‹¤.**  
ì´ëŸ¬í•œ Batch Gradient Descentì˜ ì¥ì ê³¼ ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

**ì¥ì **
1. ì „ì²´ í•™ìŠµë°ì´í„°ì— ëŒ€í•´ í•œë²ˆì˜ ì—…ë°ì´íŠ¸ê°€ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ì „ì²´ì ì¸ ì—°ì‚°íšŸìˆ˜ê°€ ì ë‹¤.
2. ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ Gradientë¥¼ ê³„ì‚°í•˜ì—¬ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì—, ìµœì ìœ¼ë¡œì˜ ìˆ˜ë ´ì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ëœë‹¤.

**ë‹¨ì **
1. í•œ Stepì— ëª¨ë“  í•™ìŠµë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.
2. ì§€ì—­ ìµœì í™”(Local Optimal)ìƒíƒœê°€ ë˜ë©´ ë¹ ì ¸ë‚˜ì˜¤ê¸° í˜ë“¤ë‹¤.
3. ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ì—…ë°ì´í„°ê°€ ì´ë£¨ì–´ì§€ê¸° ì „ê¹Œì§€ ëª¨ë“  í•™ìŠµë°ì´í„°ì— ëŒ€í•´ ì €ì¥í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë‹¤.

### Change Learnig Rate


```python
theta_path_bgd = []

def plot_gradient_descent(theta, eta, theta_path=None):
    m = len(X_b)
    plt.plot(X, y, "b.")
    n_iterations = 1000
    for iteration in range(n_iterations):
        if iteration < 10:
            y_predict = X_new_b.dot(theta)
            style = "b-" if iteration > 0 else "r--"
            plt.plot(X_new, y_predict, style)
        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
        theta = theta - eta * gradients
        if theta_path is not None:
            theta_path.append(theta)
    plt.xlabel("$x_1$", fontsize=18)
    plt.axis([0, 2, 0, 15])
    plt.title(r"$\eta = {}$".format(eta), fontsize=16)
```


```python
np.random.seed(42)
theta = np.random.randn(2,1)  # random initialization

plt.figure(figsize=(10,4))
plt.subplot(131); plot_gradient_descent(theta, eta=0.02)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)
plt.subplot(133); plot_gradient_descent(theta, eta=0.5)

save_fig("gradient_descent_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_16_1.png)


Learning Rateë¥¼ ë³€ê²½í•´ê°€ë©´ì„œ Modelì´ Fittingë˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤.

- <span>$$\eta = 0.02$$</span>: Learning Rateê°€ ì‘ê¸° ë•Œë¬¸ì—, ìˆ˜ë ´í•˜ê¸° í˜ë“¤ë‹¤.
- <span>$$\eta = 0.1$$</span>: Learning Rateê°€ ì ì ˆí•˜ì—¬, Modelì´ ì˜ Fittingëœë‹¤.
- <span>$$\eta = 0.5$$</span>: LearningRateê°€ ë„ˆë¬´ ì»¤ì„œ Global Optimumì— ìˆ˜ë ´í•˜ì§€ ëª»í•˜ê³  ë„˜ì–´ê°€ê²Œ ëœë‹¤.

ì´ëŸ¬í•œ Learning RateëŠ” HyperParameterë¡œì„œ ì‚¬ìš©ìê°€ ì§€ì •í•˜ì•  í•œë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ì— ë”°ë¼ Modelì˜ Fittingì´ ë‹¬ë¼ì§€ê²Œ ë˜ë¯€ë¡œ, ì¤‘ìš”í•œ Hyperparameterë¼ê³  í•  ìˆ˜ ìˆë‹¤.**

Learning Rateë¥¼ ì§€ì •í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œëŠ” Learning Scheduleì´ ìˆë‹¤.  

**Learning Scheduleì€ ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ì¡´ì¬í•˜ê²Œ ë˜ëŠ”ë° í˜„ì¬ ì±…ì—ì„œ ì§„í–‰í•˜ëŠ” ë°©ë²•ì€ ì²˜ìŒì—ëŠ” ì˜ ìˆ˜ë ´í•˜ì§€ ì•Šì•˜ì„ ê²ƒì´ë¼ê³  ê°€ì •í•˜ê³  Learning Rateë¥¼ í¬ê²Œ ì„¤ì •í•˜ê³  ì ì°¨ì ìœ¼ë¡œ ì¤„ì—¬ê°€ëŠ” ë°©ë²•ì´ë‹¤.**

í˜„ì¬ëŠ” Parmeterë³„ë¡œ Learning Rateë¥¼ ë‹¤ë¥´ê²Œ í•˜ì—¬ ì§„í–‰í•˜ê±°ë‚˜, Sin, Così¸ ë“±ìœ¼ë¡œ ëŠ˜ë ¸ë‹¤ ì¤„ì˜€ë‹¤ í•˜ëŠ” ë°©ì‹ ë“±ìœ¼ë¡œ ë§ì€ ì¢‹ì€ í•´ê²°ì±…ì„ ì–»ê¸° ìœ„í•˜ì—¬ ì—°êµ¬ë˜ê³  ìˆëŠ” ë¶„ì•¼ì´ë‹¤.

## Stochastic Gradient

**Stochastic Gradientë€ ëœë¤í•˜ê²Œ Dataë¥¼ Samplingí•˜ì—¬ Weight Updateí•˜ëŠ” ê²ƒ ì´ë‹¤. ëœë¤ìœ¼ë¡œ Samplingí•˜ë¯€ë¡œ Batch Gradient Descentì˜ ì¥ì ì¸ ì—°ì‚°íšŸìˆ˜ê°€ ì ì€ê²ƒ, ì•ˆì •ì ìœ¼ë¡œ Global Optimumì„ ì°¾ì„ ìˆ˜ëŠ” ì—†ì§€ë§Œ Local Optimumì— ë¹ ì§ˆ í™•ë¥ ì€ ë‚®ì„ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.**


```python
theta_path_sgd = []
m = len(X_b)
np.random.seed(42)

n_epochs = 50
t0, t1 = 5, 50  # learning schedule hyperparameters

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)  # random initialization

for epoch in range(n_epochs):
    for i in range(m):
        if epoch == 0 and i < 20:                    # not shown in the book
            y_predict = X_new_b.dot(theta)           # not shown
            style = "b-" if i > 0 else "r--"         # not shown
            plt.plot(X_new, y_predict, style)        # not shown
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
        theta_path_sgd.append(theta)                 # not shown

plt.plot(X, y, "b.")                                 # not shown
plt.xlabel("$x_1$", fontsize=18)                     # not shown
plt.ylabel("$y$", rotation=0, fontsize=18)           # not shown
plt.axis([0, 2, 0, 15])                              # not shown
save_fig("sgd_plot")                                 # not shown
plt.show()                                           # not shown
```



![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_20_1.png)



```python
print('Paramter: ğœƒ0 = {}, ğœƒ1 = {}'.format(theta[0], theta[1]))
```

    Paramter: ğœƒ0 = [4.21076011], ğœƒ1 = [2.74856079]


Scikit Learnì—ì„œ ì œê³µí•˜ëŠ” SGDRegressorë¡œì„œ Modelì„ Fittingí•˜ì—¬ë„ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.


```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)
sgd_reg.fit(X, y.ravel())

print('Paramter: ğœƒ0 = {}, ğœƒ1 = {}'.format(sgd_reg.intercept_, sgd_reg.coef_))
```

    Paramter: ğœƒ0 = [4.24365286], ğœƒ1 = [2.8250878]


## Mini-batch gradient descent
SGD + BGDë¥¼ í•©ì¹œ ë°©ë²•ì´ë‹¤.  
**ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë¯€ë¡œ, Dataë¥¼ ì ë‹¹í•œ í¬ê¸°ì˜ Batchë¡œì„œ SGDë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, SGD-1 sample, BGD-All sample, MBD-Batch sampleì¸ ê²ƒ ì´ë‹¤.**

SGDì˜ ì¥ì (Local Optimumì— ë¹ ì§ˆ í™•ë¥ ì´ ì ë‹¤.)ê³¼ BGD(ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¥´ë©° ì•ˆì •ì ì´ë‹¤.)ì˜ ì¥ì ì„ ê°™ì´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤.


```python
theta_path_mgd = []

n_iterations = 50
minibatch_size = 20

np.random.seed(42)
theta = np.random.randn(2,1)  # random initialization

t0, t1 = 200, 1000
def learning_schedule(t):
    return t0 / (t + t1)

t = 0
for epoch in range(n_iterations):
    shuffled_indices = np.random.permutation(m)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0, m, minibatch_size):
        t += 1
        xi = X_b_shuffled[i:i+minibatch_size]
        yi = y_shuffled[i:i+minibatch_size]
        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(t)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta)
        
print('Paramter: ğœƒ0 = {}, ğœƒ1 = {}'.format(theta[0], theta[1]))
```

    Paramter: ğœƒ0 = [4.25214635], ğœƒ1 = [2.7896408]


## Compare BGD vs SGD vs MGD
- BGD: ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ê³ , Iterationì˜ ìˆ˜ê°€ ì ë‹¤.
- SGD: ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ ì•Šì§€ë§Œ, Local Optimumì—ì„œ ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.
- MGD: BGD + SGD

ì•„ë˜ì˜ Modelì€ Complexityê°€ ì ê¸° ë•Œë¬¸ì—, ì „ë¶€ Global Optimumì— ìˆ˜ë ´í•œë‹¤.


```python
theta_path_bgd = np.array(theta_path_bgd)
theta_path_sgd = np.array(theta_path_sgd)
theta_path_mgd = np.array(theta_path_mgd)

plt.figure(figsize=(7,4))
plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], "r-s", linewidth=1, label="Stochastic")
plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], "g-+", linewidth=2, label="Mini-batch")
plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], "b-o", linewidth=3, label="Batch")
plt.legend(loc="upper left", fontsize=16)
plt.xlabel(r"$\theta_0$", fontsize=20)
plt.ylabel(r"$\theta_1$   ", fontsize=20, rotation=0)
plt.axis([2.5, 4.5, 2.3, 3.9])
save_fig("gradient_descent_paths_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_27_1.png)


## Polynomial regression

Polynomial regressionì´ë€ ë‹¤ì°¨ì›ìœ¼ë¡œì„œ Dataë¥¼ ë³€í˜•í•˜ì—¬ Predictioní•˜ëŠ” ë°©ë²•ì´ë‹¤.

<a href="https://wjddyd66.github.io/machine%20learning/Theory(6)Training-Testing-and-Regularization/">Training, Testing and Regularization</a>ë¥¼ ì‚´í´ë³´ë©´, Dataë¥¼ Fittingí•˜ê¸° ìœ„í•˜ì—¬ ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ Modelì˜ Complexityë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ë²•ì´ë‹¤.

Modelì˜ Complexityë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ì„œëŠ” <span>$$\theta$$</span>ì˜ ê°œìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒ ì´ê³ , ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ Polynomial Regression Modelì„ êµ¬ì„±í•œë‹¤.

<p>$$y = \theta_2x^2 + \theta_1x + \theta_0$$</p>

**Dataset**


```python
import numpy as np
import numpy.random as rnd

np.random.seed(42)

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)

plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3, 3, 0, 10])
save_fig("quadratic_data_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_31_1.png)


<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">PolynomialFeatures</a>ë¥¼ í™œìš©í•˜ì—¬ x -> x,x^2ìœ¼ë¡œì„œ ë³€í˜•í•œë‹¤.


```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
```

Polynomial regression


```python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_

print('Paramter: ğœƒ0 = {}, ğœƒ1 = {}, ğœƒ2 = {}'.format(lin_reg.intercept_[0], lin_reg.coef_[:,0][0], lin_reg.coef_[:,1][0]))
print('Real: ğœƒ0 = {}, ğœƒ1 = {}, ğœƒ2 = {}'.format(2, 1, 0.5))
```

    Paramter: ğœƒ0 = 1.7813458120291452, ğœƒ1 = 0.9336689322536068, ğœƒ2 = 0.5645626336170754
    Real: ğœƒ0 = 2, ğœƒ1 = 1, ğœƒ2 = 0.5


Visualization


```python
X_new=np.linspace(-3, 3, 100).reshape(100, 1)
X_new_poly = poly_features.transform(X_new)
y_new = lin_reg.predict(X_new_poly)
plt.plot(X, y, "b.")
plt.plot(X_new, y_new, "r-", linewidth=2, label="Predictions")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.legend(loc="upper left", fontsize=14)
plt.axis([-3, 3, 0, 10])
save_fig("quadratic_predictions_plot")
plt.show()
```



![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_37_1.png)


### Change Complexity
ìœ„ì—ì„œ ì–¸ê¸‰í•˜ì˜€ë“¯ì´, Modelì˜ Complexityê°€ ì¦ê°€í•¨ì— ë”°ë¼ì„œ Modelì˜ Biasê°€ ì¤„ê³ , Varianceê°€ ì¦ê°€í•¨ì— ë”°ë¼ì„œ Overfittingì´ ë°œìƒí•˜ê²Œ ëœë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ Modelì˜ Complexity(Num of wiehgt)ë¥¼ 1,2,300ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©´ì„œ í™•ì¸í•˜ì˜€ë‹¤.

Complexityê°€ ì¦ê°€ë¨ì— ë”°ë¼ì„œ Trainning Dataì— ëŒ€í•œ Predictionì€ ì ì  ì¦ê°€í•˜ì§€ë§Œ(BiasëŠ” ê°ì†Œ), Test Dataì— ëŒ€í•œ Predictionì´ ê°ì†Œë  ê²ƒ ì´ë‹¤.(Varianceê°€ ì¦ê°€) => Overfitting

**ì°¸ì¡°**  
- <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">StandardScaler</a>: z = (x-u)/s => Z-Normalization
- <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">Pipeline</a>: ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰


```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)

X_new=np.linspace(-3, 3, 100).reshape(100, 1)


for style, width, degree in (("g-", 1, 300), ("b--", 2, 2), ("r-+", 2, 1)):
    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)
    std_scaler = StandardScaler()
    lin_reg = LinearRegression()
    polynomial_regression = Pipeline([
            ("poly_features", polybig_features),
            ("std_scaler", std_scaler),
            ("lin_reg", lin_reg),
        ])
    polynomial_regression.fit(X, y)
    y_newbig = polynomial_regression.predict(X_new)
    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)

plt.plot(X, y, "b.", linewidth=3)
plt.legend(loc="upper left")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3, 3, 0, 10])
save_fig("high_degree_polynomials_plot")
plt.show()
```



![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_39_1.png)


ìœ„ì˜ ê²°ê³¼ë¡œëŠ” Trainning Dataì— ëŒ€í•œ, Model Fittingì˜ ê²°ê³¼ì´ë‹¤.

ë”°ë¼ì„œ, Varianceë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•˜ì—¬ Validation Datasetì„ ì‚¬ìš©í•˜ì—¬ Complexityì— ë”°ë¥¸ LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.


```python
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model, X, y):
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)
    train_errors, val_errors = [], []
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))

    plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train")
    plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val")
    plt.legend(loc="upper right", fontsize=14)   # not shown in the book
    plt.xlabel("Training set size", fontsize=14) # not shown
    plt.ylabel("RMSE", fontsize=14)              # not shown
```

Model Complexity 1 => Linear Regression


```python
lin_reg = LinearRegression()
plot_learning_curves(lin_reg, X, y)
plt.axis([0, 80, 0, 3])                         # not shown in the book
save_fig("underfitting_learning_curves_plot")   # not shown
plt.show()                                      # not shown
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_43_1.png)


Model Complexity 10 => Polynomial Regression


```python
from sklearn.pipeline import Pipeline

polynomial_regression = Pipeline([
        ("poly_features", PolynomialFeatures(degree=10, include_bias=False)),
        ("lin_reg", LinearRegression()),
    ])

plot_learning_curves(polynomial_regression, X, y)
plt.axis([0, 80, 0, 3])           # not shown
save_fig("learning_curves_plot")  # not shown
plt.show()                        # not shown
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_45_1.png)


**ì˜ˆìƒí•œ ê²°ê³¼ëŒ€ë¡œ Complexityë¥¼ ë†’ì¼ ìˆ˜ë¡, Trainning Loss(Bias)ëŠ” ì¤„ê²Œ ë˜ì—ˆìœ¼ë‚˜, Trainê³¼ Valì˜ Lossì˜ ì°¨ì´(Variance)ëŠ” ì¦ê°€í•˜ê²Œ ëœë‹¤. => Bias-Variance Trade-off**

## Regularization

Modelì˜ Complexityë¥¼ ê²°ì •í•˜ê¸° ìœ„í•˜ì—¬ Weightì˜ ê°œìˆ˜ë¥¼ ì •í•˜ëŠ” ê²ƒì€ ìœ„ì—ì„œ Learning Rateì™€ ë§ˆì°¬ê°€ì§€ë¡œ Hyperparameterë¡œì„œ ì‚¬ìš©ìê°€ ì •í•´ì•¼ í•˜ëŠ” ë¬¸ì œì´ë‹¤.  

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬, **ëŒ€ë¶€ë¶„ Modelì˜ Complexityë¥¼ ì–´ëŠì •ë„ ìœ ì§€ + Overfitting ë°©ì§€ ë°©ë²•**ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.  

Overfittingì„ í”¼í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ ê°€ì¥ ëŒ€í‘œì ì¸ ê²ƒì´ <a href="https://wjddyd66.github.io/machine%20learning/Theory(6)Training-Testing-and-Regularization/#65-defination-of-regularization">Regularization</a>ì´ë‹¤.

### Ridge regularization

**Ridge Regularization = L2 Regularization**  
<span>$$E(w) = \frac{1}{2}\sum_{n=0}^{N}(train_n - g(x_n,w))^2 + \frac{\lambda}{2}||w||^2$$</span>

**ìœ„ì˜ ë§í¬ë¥¼ ì°¸ì¡°í•˜ê²Œ ë˜ë©´, L2 Regularizationì€ Weightê°€ Gaussian Distributionì´ë¼ëŠ” Proior Knowledgeë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ê³ , ì•ì˜ <span>$$\lambda$$</span>ê°€ ì»¤ì§ˆìˆ˜ë¡ Weightê°€ ë”ìš±ë” Gaussian Distributioní˜•íƒœê°€ ë  ê²ƒì´ê³ , Weightê°€ 0ê°’ì— ê°€ê¹Œì›Œì§€ëŠ” ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤.**


```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler

# Dataset
np.random.seed(42)
m = 20
X = 3 * np.random.rand(m, 1)
y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5
X_new = np.linspace(0, 3, 100).reshape(100, 1)

# Ridge regression
def plot_model(model_class, polynomial, alphas, **model_kargs):
    for alpha, style in zip(alphas, ("b-", "g--", "r:")):
        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()
        if polynomial:
            model = Pipeline([
                    ("poly_features", PolynomialFeatures(degree=10, include_bias=False)),
                    ("std_scaler", StandardScaler()),
                    ("regul_reg", model),
                ])
        model.fit(X, y)
        y_new_regul = model.predict(X_new)
        lw = 2 if alpha > 0 else 1
        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r"$\alpha = {}$".format(alpha))
    plt.plot(X, y, "b.", linewidth=3)
    plt.legend(loc="upper left", fontsize=15)
    plt.xlabel("$x_1$", fontsize=18)
    plt.axis([0, 3, 0, 4])

plt.figure(figsize=(8,4))
plt.subplot(121)
plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(122)
plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)

save_fig("ridge_regression_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_48_1.png)


ìœ„ì˜ ê²°ê³¼ëŠ” Modelì˜ Complexity 1, 10ìœ¼ë¡œì„œ ë‚˜ëˆ„ê³  ê°ê°ì˜ RidgeRegressionì˜ <span>$$\lambda$$</span>ê°’ì„ ë³€í˜•í•´ ê°€ë©´ì„œ, Modelì˜ Fittingì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³¸ ê²ƒ ì´ë‹¤.

<span>$$\lambda$$</span>ê°’ì´ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ weight regularizationì´ ê°•í•´ì ¸ì„œ Modelì˜ Complexityê°€ ì‘ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

### Lasso Regularization
**Lasso Regularization = L1 Regularization**

<span>$$E(w) = \frac{1}{2}\sum_{n=0}^{N}(train_n - g(x_n,w))^2 + \lambda||w||$$</span>

Lasso Regularizationì€ Weightì˜ Distributionì´ Laplacian Distributionì´ë¼ëŠ” Prior Knowledgeë¥¼ ì£¼ëŠ” ë°©ë²•ì´ë‹¤.

**Ridge Regularizationê³¼ ëª©ì ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, weightë¥¼ 0ì— ê°€ê¹Œìš´ ê°’ì´ ì•„ë‹ˆë¼, 0ìœ¼ë¡œì„œ ë§Œë“¤ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ë‹¤ë¥¸ ì°¨ì´ì ì´ë‹¤. ë¹„êµí•œ ì‚¬ì§„ì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.**

<img src="https://miro.medium.com/max/602/1*o6H_R3Do1zpch-3MZk_fjQ.png" height="250" width="600">


```python
from sklearn.linear_model import Lasso

plt.figure(figsize=(8,4))
plt.subplot(121)
plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(122)
plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)

save_fig("lasso_regression_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_51_2.png)


ì´ëŸ¬í•œ Ridge Regularization, Lasso Regularizationì€ scikit learnì—ì„œ penaltyì˜ optionìœ¼ë¡œì„œ ê°„ë‹¨íˆ ì •ì˜í•  ìˆ˜ ìˆë‹¤.


```python
# Ridge Regularization
sgd_reg = SGDRegressor(penalty="l2", max_iter=1000, tol=1e-3, random_state=42)
sgd_reg.fit(X, y.ravel())
print('Ridge: ',sgd_reg.predict([[1.5]]))

# Lasso Regularization
sgd_reg = SGDRegressor(penalty="l1", max_iter=1000, tol=1e-3, random_state=42)
sgd_reg.fit(X, y.ravel())
print('Lasso: ', sgd_reg.predict([[1.5]]))
```

    Ridge:  [1.47012588]
    Lasso:  [1.47011206]


### ElasticNet
**ElasticNet = Ridge Regularization + Lasso Regularization**

<span>$$E(w) = \frac{1}{2}\sum_{n=0}^{N}(train_n - g(x_n,w))^2 + r\lambda||w|| + \frac{1-r}{2}\lambda||w||^2$$</span>


```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
elastic_net.fit(X, y)
print('Elastic: ', elastic_net.predict([[1.5]]))
```

    Elastic:  [1.54333232]


## Early Stopping
Trainning Datasetìœ¼ë¡œì„œ í›ˆë ¨ì‹œí‚¤ëŠ” Modelê³¼ Validation Datasetìœ¼ë¡œì„œ í›ˆë ¨ì‹œí‚¤ëŠ” Modelì˜ Lossë¥¼ ì¸¡ì •í•˜ë©´ì„œ, ì´ Lossì˜ ì°¨ì´ê°€ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ê³³ì—ì„œ Stoppingí•˜ëŠ” ë°©ë²•ì´ë‹¤.

ì¦‰, Modelì´ Overfittingì„ ì‹œì‘í•˜ë ¤í•  ë•Œ, Modelì˜ Trainningì„ ê·¸ë§Œí•˜ëŠ” ê²ƒ ì´ë‹¤. => Modelì˜ Functional Approximationì—ì„œëŠ” ë¶€ì¡±í•˜ì§€ë§Œ, Generalizationì—ì„œëŠ” ë” ê°•ì ì„ ë³´ì—¬ì£¼ëŠ” Modelì´ ë  ê²ƒ ì´ë‹¤.


```python
from sklearn.base import clone

# Dataset
np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

# Split Train, Vla Dataset
X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)

# For Polynomial Model
poly_scaler = Pipeline([
        ("poly_features", PolynomialFeatures(degree=90, include_bias=False)),
        ("std_scaler", StandardScaler())
    ])

X_train_poly_scaled = poly_scaler.fit_transform(X_train)
X_val_poly_scaled = poly_scaler.transform(X_val)

# SGD Regression
sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                       penalty=None, learning_rate="constant", eta0=0.0005, random_state=42)

n_epochs = 500
train_errors, val_errors = [], []

# Trainning
for epoch in range(n_epochs):
    sgd_reg.fit(X_train_poly_scaled, y_train)
    y_train_predict = sgd_reg.predict(X_train_poly_scaled)
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    train_errors.append(mean_squared_error(y_train, y_train_predict))
    val_errors.append(mean_squared_error(y_val, y_val_predict))

# Model with the least Overfitting
best_epoch = np.argmin(val_errors)
best_val_rmse = np.sqrt(val_errors[best_epoch])

# Visualirtion Model
plt.annotate('Best model',
             xy=(best_epoch, best_val_rmse),
             xytext=(best_epoch, best_val_rmse + 1),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.05),
             fontsize=16,
            )

best_val_rmse -= 0.03  # just to make the graph look better
plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], "k:", linewidth=2)
plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="Validation set")
plt.plot(np.sqrt(train_errors), "r--", linewidth=2, label="Training set")
plt.legend(loc="upper right", fontsize=14)
plt.xlabel("Epoch", fontsize=14)
plt.ylabel("RMSE", fontsize=14)
save_fig("early_stopping_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_57_1.png)


## Logistic Regression

<a href="https://wjddyd66.github.io/machine%20learning/Theory(4)Logistic-Regression/">Logistic Regression</a>ì„ ì‚´í´ë³´ê²Œ ë˜ë©´, Linear Regressionë³´ë‹¤ Logistic Regressionìœ¼ë¡œì„œ Modelì„ Fittingí•˜ê²Œ ë˜ë©´, Bayes Riskë¥¼ ì¤„ì´ë©´ì„œ, ë” ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. (Weight Updateë˜í•œ, ìœ„ì˜ ë§í¬ì— ë‚˜ì™€ìˆìŠµë‹ˆë‹¤.)

**Iris Dataset**


```python
from sklearn import datasets
iris = datasets.load_iris()
list(iris.keys())
```




    ['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']



**Model Trainning & Visualization**


```python
from sklearn.linear_model import LogisticRegression

X = iris["data"][:, 3:]  # petal width
y = (iris["target"] == 2).astype(np.int)  # 1 if Iris virginica, else 0

log_reg = LogisticRegression(solver="lbfgs", random_state=42)
log_reg.fit(X, y)

X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)

plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica")
plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2, label="Not Iris virginica")
```




    [<matplotlib.lines.Line2D at 0x7fa446e9b340>]




![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_63_1.png)


**Decision Boundary**  
Decision Boundaryë¥¼ ì‚´í´ë³´ë©´, Petal width 1.64cmë¥¼ ê¸°ì¤€ìœ¼ë¡œ Iris or Not Irisë¥¼ Classificationì„ í•˜ê²Œ ëœë‹¤.


```python
X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]

plt.figure(figsize=(8, 3))
plt.plot(X[y==0], y[y==0], "bs")
plt.plot(X[y==1], y[y==1], "g^")
plt.plot([decision_boundary, decision_boundary], [-1, 2], "k:", linewidth=2)
plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica")
plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2, label="Not Iris virginica")
plt.text(decision_boundary+0.02, 0.15, "Decision  boundary", fontsize=14, color="k", ha="center")
plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')
plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')
plt.xlabel("Petal width (cm)", fontsize=14)
plt.ylabel("Probability", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 3, -0.02, 1.02])
save_fig("logistic_regression_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_65_1.png)



```python
print('Decision Boundary: ',decision_boundary)
print('Model Prediction: {} -> {}, {} -> {}'.format(1.7, log_reg.predict([[1.7]])[0], 1.5, log_reg.predict([[1.5]])[0]))
```

    Decision Boundary:  [1.66066066]
    Model Prediction: 1.7 -> 1, 1.5 -> 0


**Decision Boundary Visualization**  

ì‹¤ì œ Decision Boundaryì— ëŒ€í•˜ì—¬ Visualizationì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê²Œ ëœë‹¤.  
ì‹¤ì œ Logistic Regressionì˜ Modelì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
<p>$$\sigma(\theta_0*\text{Petal width} + \theta_1*\text{Petal length}+\theta_2)\text{,  }\sigma \text{: Sigmoid Function}$$</p>

ìœ„ì˜ Logistic Regressionì„ Logit ë³€í™˜ì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
<p>$$\theta_0*\text{Petal width} + \theta_1*\text{Petal length}+\theta_2$$</p>
ì¦‰, Petal width, Petal lengthì˜ Dimensionì—ì„œ ë³´ì•˜ì„ ê²½ìš°, Linear í•œ Lineìœ¼ë¡œì„œ Decisionì„ íŒë‹¨í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤.

ì•„ë˜ ê²°ê³¼ëŠ” virginicaì¼ í™•ë¥ ì— ëŒ€í•˜ì—¬ Lineì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ ì´ë‹¤.


```python
from sklearn.linear_model import LogisticRegression

X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.int)

log_reg = LogisticRegression(solver="lbfgs", C=10**10, random_state=42)
log_reg.fit(X, y)

x0, x1 = np.meshgrid(
        np.linspace(2.9, 7, 500).reshape(-1, 1),
        np.linspace(0.8, 2.7, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]

y_proba = log_reg.predict_proba(X_new)

plt.figure(figsize=(10, 4))
plt.plot(X[y==0, 0], X[y==0, 1], "bs")
plt.plot(X[y==1, 0], X[y==1, 1], "g^")

zz = y_proba[:, 1].reshape(x0.shape)
contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)


left_right = np.array([2.9, 7])
boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]

plt.clabel(contour, inline=1, fontsize=12)
plt.plot(left_right, boundary, "k--", linewidth=3)
plt.text(3.5, 1.5, "Not Iris virginica", fontsize=14, color="b", ha="center")
plt.text(6.5, 2.3, "Iris virginica", fontsize=14, color="g", ha="center")
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.axis([2.9, 7, 0.8, 2.7])
save_fig("logistic_regression_contour_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_68_1.png)


## Softmax Regression
ìœ„ì˜ Logistic Regressionì€ Binary Classë¥¼ Classficationí•˜ëŠ” Modelì´ë‹¤. ìœ„ì˜ ë¬¸ì œëŠ” Iris Datasetì— ëŒ€í•˜ì—¬ virginica or not virginicaë¥¼ Classificationí•˜ëŠ” ì˜ˆì‹œì˜€ë‹¤.  
SigmoidëŠ” 0~1ì˜ Outputìœ¼ë¡œì„œ 0.5ë¥¼ thresholdë¡œì„œ ì •ì˜í•˜ì—¬ Binary Classifictionì´ ê°€ëŠ¥í•˜ë‹¤.

**Softmax Regressionì€ Binary Classificationì´ ì•„ë‹ˆë¼, Multi Class Classificationì„ í•˜ëŠ” Modelì´ë‹¤. Sigmoid Functionì´ ì•„ë‹Œ Softmax Functionìœ¼ë¡œì„œ Modelì„ ë§Œë“¤ê²Œ ëœë‹¤.**




```python
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = iris["target"]

softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10, random_state=42)
softmax_reg.fit(X, y)

x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]


y_proba = softmax_reg.predict_proba(X_new)
y_predict = softmax_reg.predict(X_new)

zz1 = y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)

plt.figure(figsize=(10, 4))
plt.plot(X[y==2, 0], X[y==2, 1], "g^", label="Iris virginica")
plt.plot(X[y==1, 0], X[y==1, 1], "bs", label="Iris versicolor")
plt.plot(X[y==0, 0], X[y==0, 1], "yo", label="Iris setosa")

from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
save_fig("softmax_regression_contour_plot")
plt.show()
```


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Ch4.Training_Linear_Models_files/Ch4.Training_Linear_Models_70_1.png)

<hr>
ì°¸ì¡°: <a href="https://github.com/wjddyd66/HandsOn/blob/master/Ch4.Training_Linear_Models.ipynb">ì›ë³¸ì½”ë“œ</a><br>
ì½”ë“œì— ë¬¸ì œê°€ ìˆê±°ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ wjddyd66@naver.comìœ¼ë¡œ  Mailì„ ë‚¨ê²¨ì£¼ì„¸ìš”.

