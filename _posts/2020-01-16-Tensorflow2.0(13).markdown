---
layout: post
title:  "CycleGAN"
date:   2020-01-20 10:00:20 +0700
categories: [Tnesorflow2.0]
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
### Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
Code ì°¸ì¡°: <a href="https://www.tensorflow.org/tutorials/generative/cyclegan?hl=ko">CycleGAN</a><br>
ë…¼ë¬¸ ì°¸ì¡°: <a href="https://arxiv.org/pdf/1703.10593.pdf">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a><br>

#### (1) Introduction
ëŒ€ë¶€ë¶„ì˜ Generate Paperì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì²˜ìŒì˜ ì‚¬ì§„ìœ¼ë¡œì„œ í•´ë‹¹ Paperì˜ ëª©ì ì„ ê°•ë ¥í•˜ê²Œ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/50.png" height="100%" width="100%" /></div><br>
ìœ„ì˜ ì‚¬ì§„ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ **CycleGANì˜ ìµœì¢…ì ì¸ ëª©ì ì€ ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜ì„ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒ ì´ë‹¤. ì¦‰, Image to Image Translationì˜ í•œ ì¢…ë¥˜ì´ë‹¤.**  

ì´ëŸ¬í•œ CycleGANì˜ Abstractì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•˜ê³  ìˆë‹¤.  
>...
However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples.  
...  
Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y â†’ X and introduce a cycle consistency loss to enforce F(G(X)) â‰ˆ X (and vice versa). 
>

ìœ„ì˜ ë‘ ë¬¸ì¥ì´ í•´ë‹¹ ë…¼ë¬¸ Abstractì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°ë˜ëŠ” ë¶€ë¶„ì´ë‹¤.  
1) Image-to-Image Translationì—ì„œëŠ” Paired-Image Setìœ¼ë¡œì„œ Trainingì„ ë§ì´ ì§„í–‰í•œë‹¤.(ex) <a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(1)/">Pix2Pix</a>)  
í•˜ì§€ë§Œ ì´ëŸ¬í•œ Paired-Datasetì„ ë§Œë“œëŠ” ê²ƒì€ í˜„ì‹ì ìœ¼ë¡œ ë§ì´ ì–´ë µê¸° ë•Œë¬¸ì— **Unpaired Datasetìœ¼ë¡œì„œ Modelì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì–˜ê¸°í•œë‹¤.** Unpaired Datasetì˜ ì˜ˆì‹œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/51.png" height="100%" width="100%" /></div><br>
2) Modelì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œì„œ **cycle consistency loss**ì„ ì‚¬ìš©í•˜ì—¬ Modelì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.  
X,Yë¥¼ ì˜ˆë¡œë“¤ì–´ì„œ GANì„ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.(G: Generator, D: Discriminator)  
- <span>$$G(X) \rightarrow 1$$</span>
- <span>$$D(G(X)) \rightarrow 0 \text{  ,  }D(Y) \rightarrow 1$$</span>

ì¦‰, GeneratorëŠ” Discriminatorê°€ ì‹¤ì œ Dataì™€ ìì‹ ì´ ìƒì„±í•œ Dataë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•˜ê²Œ í•™ìŠµì‹œí‚¨ë‹¤.  
**í•´ë‹¹ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ GANì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•˜ê³  ìˆë‹¤.**  
>The optimal G thereby translates the domain X to a domain YË† distributed identically to Y.  
However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way â€“ there are infinitely many mappings G that will induce the same distribution over yË†. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the wellknown problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress
>

ë‹¨ìˆœí•œ Mappingìœ¼ë¡œ ì¸í•˜ì—¬ Label Domainì²˜ëŸ¼ ë³´ì´ê²Œ ë§Œë“¤ë©´ ë˜ê¸° ë•Œë¬¸ì— ë³€í•˜ê¸° ì‰¬ìš´ Labelë§Œ ë§Œë“¤ê²Œ ë  ê²ƒì´ê³ , ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë¼ê³  ë³´ê¸° í˜ë“¤ ê²ƒ ì´ë‹¤.  

**ë”°ë¼ì„œ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì£¼ì¥í•˜ëŠ” Cycle consistency loss**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.  
>Mathematically, if we have a translator G : X â†’ Y and another translator F : Y â†’ X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [64] that encourages F(G(x)) â‰ˆ x and G(F(y)) â‰ˆ y
>

**ë‹¨ìˆœíˆ Xë¥¼ Y Domainìœ¼ë¡œ ì´ë™ì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ X -> Y -> Xë¡œì„œ Reconstructionì„ í•œ ë’¤, Lossë¥¼ êµ¬í•˜ì—¬ ê³„ì‚°í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.**  
í•´ë‹¹ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë°©ë²•ì„ Cycle consistency lossë¼ê³  ì¹­í•˜ì˜€ê³  ê·¸ë¦¼ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚´ì—ˆë‹¤.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/52.png" height="100%" width="100%" /></div><br>
#### (2) Formulation
ê¸°ë³¸ì ìœ¼ë¡œ CycleGANë„ GANì´ê¸° ë•Œë¬¸ì— <a href="https://wjddyd66.github.io/pytorch/Pytorch-GAN/">GAN</a>ì—ì„œ ì‚¬ìš©í•œ Loss Functionì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
<p>$$L_{GAN}(G,D_{Y},X,Y) = \mathbb{E}_{y\text{~}p_{data}(y)}[log D_{Y}(y)]+\mathbb{E}_{x\text{~}p_{data}(x)}[1-log D_{Y}(G(x))]$$</p>
ìœ„ì™€ ê°™ì´ ì„œë¡œ ìƒë°˜ë˜ëŠ” Lossê°’ì„ ë”í•´ì£¼ëŠ” í˜•ì‹ì„ **Adversial Loss**ë¼ê³  ë¶€ë¥´ê²Œ ëœë‹¤.  
ìœ„ì˜ GANì€ ì´ëŸ¬í•œ Adversial Lossí˜•ì‹ì„ ì·¨í•˜ê²Œ ëœë‹¤.  
<br>

**ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ê³  ìˆëŠ” CycleGANì„ ìœ„í•œ Cycle Consistency Lossë¶€ë¶„ë§Œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.**  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/53.png" height="100%" width="100%" /></div><br>
ìœ„ì˜ ì‹ì„ ìœ„í•˜ì—¬ ì•ìœ¼ë¡œ ì‚¬ìš©í•  Parameterë“¤ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³  ê°™ë‹¤.  
- <span>$$\left\{ x_i \right\}_{i=1}^{N} \in X$$</span>: Training Data
- <span>$$\left\{ y_j \right\}_{j=1}^{M} \in Y$$</span>: Target Data
- <span>$$p_{data}(x)$$</span>: Training Data Distribution
- <span>$$p_{data}(y)$$</span>: Target Data Distribution
- <span>$$G: X -> Y$$</span>: Training Data -> Target Data (Generator)
- <span>$$F: Y -> X$$</span>: Target Data -> Training Data (Generator)
- <span>$$D_{Y}: D_{Y}(G(X))=0 \text{  ,  }D_{X}(Y)=1$$</span>: Target Data(Y)ëŠ” 1ë¡œì„œ íŒë³„, <span>$$G(X)$$</span>ëŠ” 0ìœ¼ë¡œì„œ íŒë³„ (Discriminator)
- <span>$$D_{X}: D_{X}(F(Y))=0 \text{  ,  }D_{X}(X)=1$$</span>: Training Data(X)ëŠ” 1ë¡œì„œ íŒë³„, <span>$$F(Y)$$</span>ëŠ” 0ìœ¼ë¡œì„œ íŒë³„ (Discriminator)

ìœ„ì™€ ê°™ì´ Parameterë“¤ì„ ì •ì˜í•˜ê²Œ ë˜ë©´ <span>$$F(G(x)) \simeq x \text{  ,  }G(F(y)) \simeq y$$</span>ì˜ ì‹ì´ ì„±ë¦½í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  

ìœ„ì™€ ê°™ì€ ì‹ì¼ê²½ìš° **Cycle Consistency Loss**ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.  
<p>$$L_{cyc}(G,F) = \mathbb{E}_{x\text{~}p_{data}(x)}[||F(G(x))-x||_{1}]+\mathbb{E}_{y\text{~}p_{data}(y)}[||G(F(y))-y||_{1}]$$</p>
**ì¦‰, ë‹¨ìˆœí•œ Mappingìœ¼ë¡œ ì¸í•˜ì—¬ Label Domainì²˜ëŸ¼ ë³´ì´ê²Œ ë§Œë“¤ë©´ ë˜ê¸° ë•Œë¬¸ì— ë³€í•˜ê¸° ì‰¬ìš´ Labelë§Œ ë§Œë“¤ê²Œ ë˜ëŠ” ê²ƒì´ ì•„ë‹Œ Reconstuctionì„ í†µí•˜ì—¬ ì‹¤ì œ Training, Target Dataì™€ì˜ L1 Lossë¡œ ì¸í•˜ì—¬ ì¢€ ë” Target Dataì™€ ë¹„ìŠ·í•œ Imageê°€ ìƒì„±ëœë‹¤ê³  ì˜ˆìƒí•  ìˆ˜ ìˆë‹¤.**  

ìµœì¢…ì ì¸ Loss ObjectëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.  
<p>$$L(G,F,D_{X},D_{Y}) = L_{GAN}(G,D_{Y},X,Y) + L_{GAN}(F,D_{X},Y,X) + \lambda L_{cyc}(G,F)$$</p>
<p>$$G^{*},F^{*} =arg min_{G,F} max_{G_{X},D_{Y}} L(G,F,D_{X},D_{Y})$$</p>
#### (3) CycleGan vs Pix2Pix
<a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(1)/">Pix2Pix</a>ì—ì„œ Pix2Pixì˜ LossFunctionì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•˜ì˜€ë‹¤.  

<p>$$L_{L1}(G) = \mathbb{E}_{x,y,z}[||y-G(x,z)||_1]$$</p>
<p>$$G^{*} = \text{arg } \underset{G}{min} \underset{D}{max} L_{cGAN}(G,D) + \lambda L_{L1}(G)$$</p>
**CNN L1 Loss Function(MSE) Modelê²°ê³¼ëŠ” BlurFilterë¥¼ ì ìš©ì‹œí‚¨ ê°™ì€ ê²°ê³¼ë¡œì„œ Imageì˜ Low-Frequencyë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤.**  
**CGAN Modelê²°ê³¼ëŠ” Figh Frequencyë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤.**  
ë”°ë¼ì„œ Imageì˜ Low Frequencyì™€ High Frequencyë¥¼ ì „ë¶€ í•™ìŠµí•˜ì—¬ ì¢€ ë” Sharpí•˜ë©´ì„œ Realisticí•œ Imageì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.  
<br>

**Pix2Pixì—ì„œ CycleGANì´ë¼ëŠ” ìƒˆë¡œìš´ ê¸°ë²•ì— ëŒ€í•œ ì¥ì ì„ ìƒê°í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.(Pix2Pix -> CycleGANìˆœìœ¼ë¡œ ê°™ì€ ê³³ì—ì„œ ë°œí‘œí•˜ì˜€ë‹¤.)**  

Pix2PixëŠ” <span>$$\lambda L_{L1}(G)$$</span>ì´ Low-Frequencyì— ëŒ€í•œ í•™ìŠµí•˜ê²Œ ë˜ë¯€ë¡œ ëŒ€ë¶€ë¶„ì˜ ImageëŠ” Low-Frequencyë¡œì„œ **Loss ObjectëŠ” L1 Lossì— ë§ì´ ì˜ì§€í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ High-Frequencyë¥¼ ë‹´ë‹¹í•˜ëŠ” <span>$$L_{cGAN}(G,D)$$</span>ì˜ ì—­í• ì„ ë§¤ìš° ë§ì´ ìƒì‹¤ëœë‹¤.**  
<br>

ë°˜ë©´ **CycleGANì˜ ì‹ì„ ì‚´í´ë³´ë©´ Pix2Pixì™€ ë§ˆì°¬ê°€ì§€ë¡œ <span>$$L_{cyc}(G,F)$$</span>ë¡œì„œ Low-Frequencyë¥¼ ë‹´ë‹¹í•˜ëŠ” ë¶€ë¶„ì´ ì¡´ì¬í•˜ì§€ë§Œ ì´ê²ƒì€ ì „ì ìœ¼ë¡œ <span>$$L_{GAN}(G,D_{Y},X,Y), L_{GAN}(F,D_{X},Y,X)$$</span>ë¡œì„œ í•™ìŠµë˜ëŠ” G, Fì— ê´€ë ¨ë˜ì–´ ìˆë‹¤. ì¦‰, High-Frequencyì™€ Low-Frequencyë¥¼ ë‘˜ ë‹¤ ì ì ˆí•˜ê²Œ ì¡°í•©ì„ ì˜ í•˜ëŠ” Image-to-Image Translation Modelì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.**  

ì¶”ê°€ì ìœ¼ë¡œ, Codeì˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©´ **GANì˜ íŠ¹ì„±ìƒ ì •í™•í•œ ìˆ˜ì¹˜ìƒìœ¼ë¡œ ì–´ë–¤ ê²ƒì´ ë” ì¢‹ì€ Modelì´ë‹¤ ë¼ê³  í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, Paired-Datasetìœ¼ë¡œì„œ Trainingë˜ëŠ” Pix2Pixë³´ë‹¤ëŠ” ê²°ê³¼ê°€ ì•ˆì¢‹ë‹¤.** ë¼ê³  ìœ¡ì•ˆìœ¼ë¡œëŠ” í™•ì¸í•  ìˆ˜ ìˆë‹¤.
<br><br>


### CycleGAN êµ¬í˜„
CycleGANì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì´ Architectureë¥¼ êµ¬ì„±í•˜ì˜€ë‹¤ê³  í•©ë‹ˆë‹¤.  
>Network Architecture We adopt the architecture for our generative networks from Johnson et al.  
[23] who have shown impressive results for neural style transfer and superresolution. This network contains two stride-2 convolutions, several residual blocks [18], and two fractionallystrided convolutions with stride 1/2.  
We use 6 blocks for 128 Ã— 128 images and 9 blocks for 256 Ã— 256 and higherresolution training images.  
Similar to Johnson et al. [23], we use instance normalization [53]. For the discriminator networks we use 70 Ã— 70 PatchGANs [22, 30, 29], which aim to classify whether 70 Ã— 70 overlapping image patches
are real or fake.  
Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion [22].  

í•˜ì§€ë§Œ í˜„ì¬ Tensorflow 2.0 Turtorialì—ì„œëŠ” ì´ì „ Post <a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(1)/#lossfunction">Pix2Pix</a>ì˜ U-Net Networkêµ¬ì¡°ë¡œì„œ í•´ë‹¹ CycleGANì„ êµ¬í˜„í•©ë‹ˆë‹¤.  
ë”°ë¼ì„œ Pix2Pixì˜ Modelì„ ë¨¼ì € Downloadë°›ê³  ì‹œì‘í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.  
<code>!pip install -q git+https://github.com/tensorflow/examples.git</code>

<br>


#### Set up the input pipeline
í•„ìš”í•œ Libraryë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ëª‡ëª‡ ì„¤ì •ì„ ì§€ì •í•œë‹¤.  
- mpl: Matplotlibì˜ ì„¤ì • ì§€ì •
- <code>tfds.disable_progress_bar()</code>: Datasetì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ìš° ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ”ë‹¤.
- <code>tf.data.experimental.AUTOTUNE</code>: Networkê°€ ìŠ¤ìŠ¤ë¡œ ì„¤ì •í•˜ê³  Datasetì„ ì˜ ë¶ˆë¡œì˜¬ ìˆ˜ ìˆê²Œ ê²°ì •í•´ë¼ Autotuneì— ëŒ€í•´ì„œ ê¶ê¸ˆí•˜ì‹œ ë¶„ì€ ë§í¬ ì°¸ì¡°. <a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(4)/#autotune">Autotune</a>

```python
from __future__ import absolute_import, division, print_function, unicode_literals
from tensorflow_examples.models.pix2pix import pix2pix
import tensorflow_datasets as tfds
import tensorflow as tf

from IPython.display import clear_output
import matplotlib.pyplot as plt
import matplotlib as mpl

import time
import os

# ì•ìœ¼ë¡œ ì‚¬ìš©í•  Matplotlibì˜ Default Optionì„ ì„¤ì •í•œë‹¤.
mpl.rcParams['figure.figsize'] = (10, 10)
mpl.rcParams['axes.grid'] = False

# tfdsë¡œì„œ Dataë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²½ìš° ìƒíƒœë¥¼ ë³´ì´ì§€ ì•Šê²Œ ì„¤ì •í•œë‹¤.
tfds.disable_progress_bar()

# Datasetì„ ì˜ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ Networkê°€ ìŠ¤ìŠ¤ë¡œ ì„¤ì •í•˜ê²Œ í•œë‹¤.
AUTOTUNE = tf.data.experimental.AUTOTUNE
```
<br>
<br><br>

#### Input Pipeline
Modelì— ë„£ê¸° ìœ„í•œ Preprocessing ê³¼ì •ì´ë‹¤. ê°ê°ì˜ Functionì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆë‹¤.  
- random_crop(): Imageë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ Hyperparameterì˜ Sizeë¡œì„œ Randomí•˜ê²Œ ìë¥¸ë‹¤.
- normalize(): Imageì˜ ê°’ì„ 0 ~ 255 -> -1 ~ 1ë¡œì„œ Normalizationí•œë‹¤.
- random_jitter(): Image Size (286,286)ë¡œì„œ ì§€ì • -> random_crop() ì ìš© -> ì¼ì •í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì„ ì‹œí‚¨ë‹¤.
- preprocess_image_train(): Train Imageë¥¼ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‹¤ì‹œí•œë‹¤. random_jitter() -> Normalization
- preprocess_image_test(): Test Imageë¥¼ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‹¤ì‹œí•œë‹¤. Normalization

ìœ„ì™€ê°™ì€ Functionì„ í™œìš©í•˜ì—¬  
```python
train_horses = train_horses.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(1)
```
<br>
ì™€ ê°™ì´ Batch Sizeê°€ 1ì´ê³  Shuffleí•œ Datasetì„ ë§Œë“ ë‹¤.

```python
# Modelì— ì ìš©í•  Datasetì„ Downloadë°›ê³ , Train(Horse, Zebra), Test(Horse, Zebra)ë¡œì„œ ë¶„í• í•œë‹¤.
dataset, metadata = tfds.load('cycle_gan/horse2zebra',
                              with_info=True, as_supervised=True)

train_horses, train_zebras = dataset['trainA'], dataset['trainB']
test_horses, test_zebras = dataset['testA'], dataset['testB']

# ì‚¬ìš©í•  Hyperparameterë¥¼ ì§€ì •í•œë‹¤.
BUFFER_SIZE = 1000
BATCH_SIZE = 1
IMG_WIDTH = 256
IMG_HEIGHT = 256

# Imageë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ Hyperparameterì˜ Sizeë¡œì„œ Randomí•˜ê²Œ ìë¥¸ë‹¤.
def random_crop(image):
    cropped_image = tf.image.random_crop(
        image, size=[IMG_HEIGHT, IMG_WIDTH, 3])

    return cropped_image

# Imageì˜ ê°’ì„ 0 ~ 255 -> -1 ~ 1ë¡œì„œ Normalizationí•œë‹¤.
def normalize(image):
    image = tf.cast(image, tf.float32)
    image = (image / 127.5) - 1
    return image

# Image Size (286,286)ë¡œì„œ ì§€ì • -> random_crop() ì ìš© -> ì¼ì •í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì„ ì‹œí‚¨ë‹¤
def random_jitter(image):
    # resizing to 286 x 286 x 3
    image = tf.image.resize(image, [286, 286],
                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)

    # randomly cropping to 256 x 256 x 3
    image = random_crop(image)

    # random mirroring(ì¢Œìš° ë°˜ì „)
    image = tf.image.random_flip_left_right(image)

    return image

# Train Imageë¥¼ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‹¤ì‹œí•œë‹¤. random_jitter() -> Normalization
def preprocess_image_train(image, label):
    image = random_jitter(image)
    image = normalize(image)
    return image

# Test Imageë¥¼ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‹¤ì‹œí•œë‹¤. Normalization
def preprocess_image_test(image, label):
    image = normalize(image)
    return image

# ìœ„ì—ì„œ ì •ì˜í•œ Functionì„ í™œìš©í•˜ì—¬ Dataset ìƒì„±
train_horses = train_horses.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(1)

train_zebras = train_zebras.map(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(1)

test_horses = test_horses.map(
    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(1)

test_zebras = test_zebras.map(
    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(1)

# Modelì— ë„£ì„ Datasetì„ ìƒì„±í•œë‹¤. 1ê°œì”© ê°€ì¡ì˜¤ê²Œ ëœë‹¤.
sample_horse = next(iter(train_horses))
sample_zebra = next(iter(train_zebras))

# DataPreprocessì˜ ê²°ê³¼ì™€ Mirroringì„ Visualizationí•œë‹¤. 
plt.subplot(221)
plt.title('Horse')
plt.axis('off')
plt.imshow(sample_horse[0] * 0.5 + 0.5)

plt.subplot(222)
plt.title('Horse with random jitter')
plt.axis('off')
plt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)

plt.subplot(223)
plt.title('Zebra')
plt.axis('off')
plt.imshow(sample_zebra[0] * 0.5 + 0.5)

plt.subplot(224)
plt.title('Zebra with random jitter')
plt.axis('off')
plt.imshow(random_jitter(sample_zebra[0]) * 0.5 + 0.5)
```
<br>
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/54.png" height="500" width="500" /></div><br>
<br><br>

#### Import and reuse the Pix2Pix models
Pix2Pixë¥¼ í™œìš©í•˜ì—¬ 2ê°œì˜ Generatorì™€ Discriminatorë¥¼ ìƒì„±í•œë‹¤.  
ìœ„ì˜ ë…¼ë¬¸ ì„¤ëª…ì—ì„œ ì–¸ê¸‰í•œ Parameterë“¤ì„ Codeì— Mappingí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
ìœ„ì˜ ì‹ì„ ìœ„í•˜ì—¬ ì•ìœ¼ë¡œ ì‚¬ìš©í•  Parameterë“¤ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³  ê°™ë‹¤.  

- <span>$$G: X -> Y$$</span>: Training Data -> Target Data (Generator) = <code>generator_g</code>
- <span>$$F: Y -> X$$</span>: Target Data -> Training Data (Generator) = <code>generator_f</code>
- <span>$$D_{X}: D_{X}(F(Y))=0 \text{  ,  }D_{X}(X)=1$$</span>: Training Data(X)ëŠ” 1ë¡œì„œ íŒë³„, <span>$$F(Y)$$</span>ëŠ” 0ìœ¼ë¡œì„œ íŒë³„ (Discriminator) = <code>discriminator_x</code>
- <span>$$D_{Y}: D_{Y}(G(X))=0 \text{  ,  }D_{Y}(Y)=1$$</span>: Target Data(Y)ëŠ” 1ë¡œì„œ íŒë³„, <span>$$G(X)$$</span>ëŠ” 0ìœ¼ë¡œì„œ íŒë³„ (Discriminator) = <code>discriminator_y</code>

**í•˜ë‚˜ ì¤‘ìš”í•˜ê²Œ ì§‘ê³  ë„˜ì–´ê°€ì•¼ ë˜ëŠ” ê²ƒì€ Pix2PixëŠ” Discriminatorì— input + targetìœ¼ë¡œì„œ CGANìœ¼ë¡œì„œ êµ¬ì„±í•˜ì˜€ëŠ”ë° ì´ë²ˆ CycleGANì—ì„œëŠ” <code>pix2pix.discriminator(norm_type='instancenorm', target=False)</code>ì—ì„œ target=False Optionì„ ì£¼ì–´ì„œ GANì„ ì‚¬ìš©í•œë‹¤.**  

```python
# Color Image Outputì„ ë½‘ì•„ë‚´ê¸° ìœ„í•œ Channel = 3
OUTPUT_CHANNELS = 3

# G,F ìƒì„±
generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')
generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')

# D_x, D_y ìƒì„±
discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)
discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)

# Generaterê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ Visualizationí•˜ê¸° ìœ„í•œ Sample
# F(G(x)) â‰ˆ x
to_zebra = generator_g(sample_horse)
# G(F(y)) â‰ˆ y
to_horse = generator_f(sample_zebra)
plt.figure(figsize=(8, 8))
contrast = 8

# Generator Visualization
imgs = [sample_horse, to_zebra, sample_zebra, to_horse]
title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']

for i in range(len(imgs)):
    plt.subplot(2, 2, i+1)
    plt.title(title[i])
    if i % 2 == 0:
        # sample_horse, sample_zebraëŠ” [-1,1]ë¡œ Normalizationë˜ì—ˆê¸° ë•Œë¬¸ì—
        # [0,1]ë¡œì„œ ê°’ì„ ë°”ê¾¸ëŠ” ê³¼ì •ì´ë‹¤.
        plt.imshow(imgs[i][0] * 0.5 + 0.5)
    else:
        
        
        # ë‚˜ì¤‘ì— Modelê²°ê³¼ í™•ì¸í›„ í™•ì¸í•´ë³´ê¸° ê°œì¸ì ìœ¼ë¡œëŠ” tanhë¥¼ ë§ˆì§€ë§‰
        # Activationìœ¼ë¡œ í•˜ì˜€ìœ¼ë¯€ë¡œ ìœ„ì™€ ê°™ì´ Normalizationí•´ë„ ë  ê²ƒ ê°™ìŒ
        plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)
plt.show()
```
<br>
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/55.png" height="500" width="500" /></div><br>
ë‹¨ìˆœí•œ Discriminatorë¥¼ Visualizationí•œ ê²ƒì´ë‹¤.  
Discriminatorì˜ ìµœì¢…ì ì¸ SizeëŠ” (Batch, 30, 30, 1)ì´ë¯€ë¡œ 30 * 30ìœ¼ë¡œì„œ ê²°ê³¼ë¥¼ Visualizationë˜ì—ˆë‹¤ëŠ” ì •ë„ë§Œ ì•Œì•„ë„ ëœë‹¤.
```python
plt.figure(figsize=(8, 8))

plt.subplot(121)
plt.title('Is a real zebra?')
plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')

plt.subplot(122)
plt.title('Is a real horse?')
plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')

plt.show()
```
<br>
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/56.png" height="500" width="500" /></div><br>
<br><br>

#### Loss functions
**ë¨¼ì € ìƒê°í•˜ê³  ê°€ì•¼ë˜ëŠ” ì ì€ í˜„ì¬ Modelì„ Pix2Pixë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒ ì´ë‹¤.**  
ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œ ì œê³µí•˜ëŠ” LossFunctionì´ ì•„ë‹Œ í•´ë‹¹ Modelì— ë§ëŠ” LossFunctionì„ ìƒˆë¡­ê²Œ êµ¬ì„±í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ ì´ë‹¤.  
ë¨¼ì € GAN, CGAN, Pix2Pix, CycleGANì˜ LossFunctionì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
**GAN**  
<p>$$L_{GAN}(G,D_{Y},X,Y) = \mathbb{E}_{y\text{~}p_{data}(y)}[log D_{Y}(y)]+\mathbb{E}_{x\text{~}p_{data}(x)}[1-log D_{Y}(G(x))]$$</p>
**CGAN**  
<p>$$L_{CGAN}(G,D_{Y},X,Y) = \mathbb{E}_{y\text{~}p_{data}(y)}[log D_{Y}(y,z)]+\mathbb{E}_{x\text{~}p_{data}(x)}[1-log D_{Y}(G(x,z),z)]$$</p>
**Pix2Pix**  
<p>$$L_{L1}(G) = \mathbb{E}_{x,y,z}[||y-G(x,z)||_1]$$</p>
<p>$$G^{*} = \text{arg } \underset{G}{min} \underset{D}{max} L_{cGAN}(G,D) + \lambda L_{L1}(G)$$</p>
**CycleGAN**  
<p>$$L_{cyc}(G,F) = \mathbb{E}_{x\text{~}p_{data}(x)}[||F(G(x))-x||_{1}]+\mathbb{E}_{y\text{~}p_{data}(y)}[||G(F(y))-y||_{1}]$$</p>
<p>$$L(G,F,D_{X},D_{Y}) = L_{GAN}(G,D_{Y},X,Y) + L_{GAN}(F,D_{X},Y,X) + \lambda L_{cyc}(G,F)$$</p>
<p>$$G^{*},F^{*} =arg \underset{G,F}{min} \underset{G_{X},D_{Y}}{max} L(G,F,D_{X},D_{Y})$$</p>
ì¦‰, ê¸°ë³¸ì ì¸ CycleGANì„ ì‚¬ìš©í•˜ë˜, Pix2Pixì˜ <span>$$L_{L1}(G)$$</span>ë˜í•œ ìƒê°í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ ì´ë‹¤.  

ë”°ë¼ì„œ ìµœì¢…ì ì¸ Loss ObjectëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ëœë‹¤.  
<p>$$L(G,F,D_{X},D_{Y}) = L_{GAN}(G,D_{Y},X,Y) + L_{GAN}(F,D_{X},Y,X) +$$</p> 
<p>$$\lambda_{1} L_{cyc}(G,F) + \lambda_{2}L_{L1}(G)+ \lambda_{2}L_{L1}(F)$$</p>
<p>$$G^{*},F^{*} =arg \underset{G,F}{min} \underset{G_{X},D_{Y}}{max} L(G,F,D_{X},D_{Y})+ \lambda_{2}L_{L1}(G)+ \lambda_{2}L_{L1}(F)$$</p>
ìœ„ì˜ ì‹ì—ì„œ <span>$$G^{*}$$</span>ì™€ <span>$$F^{*}$$</span>ëŠ” êµ¬ì„±ì´ ë˜‘ê°™ìœ¼ë¯€ë¡œ <span>$$G^{*}$$</span>ì—ëŒ€í•´ì„œë§Œ ì•Œì•„ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  

**<span>$$G^{*}$$</span> Loss Function**  
<p>$$G^{*} = arg \underset{G}{min} \underset{D_{Y}}{max} L(G,F,D_{X},D_{Y})+ \lambda_{2}L_{L1}(G)+ \lambda_{2}L_{L1}(F)$$</p>
<p>$$= arg \underset{G}{min} \underset{D_{Y}}{max} L_{GAN}(G,D_{Y},X,Y)+ \lambda_{1}L_{cyc}(G)+ \lambda_{2}L_{L1}(G)$$</p>
**Generator**  
ìœ„ì˜ ì‹ì—ì„œ Generatorì˜ ê´€ë ¨ëœ ì‹ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
<p>$$Loss_{G^{*} \text{  } Generator} = \underset{G}{max} \mathbb{E}_{x\text{~}p_{data}(x)}[log D_{Y}(G(x))] + \lambda_{1}L_{cyc}(G)+ \lambda_{2}L_{L1}(G)$$</p>
(ë§ì€ ë¶€ë¶„ì„ ìƒëµí•˜ì˜€ìŠµë‹ˆë‹¤. ì´í•´ë˜ì§€ ì•Šìœ¼ì‹œëŠ” ë¶„ë“¤ì€ <a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(1)/#generator-loss">Pix2Pix-GeneratorLoss</a>ë¥¼ ì°¸ì¡°í•˜ì‹œë©´ ë˜ê² ìŠµë‹ˆë‹¤.)  

**Discriminator**  
Discriminatorì˜ ê´€ë ¨ëœ ì‹ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
<p>$$Loss_{G^{*} \text{  } Discriminator} = \underset{D_{Y}}{max} \mathbb{E}_{y\text{~}p_{data}(y)}[log D_{Y}(y)]+\mathbb{E}_{x\text{~}p_{data}(x)}[1-log D_{Y}(G(x))]$$</p>
ì•„ë˜ Codeì—ì„œ ê°ê°ì˜ Loss Functionì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  

**discriminator_loss(real, generated)**
- real: ì‹¤ì œ Label Data Y
- generated: Generatorì—ì„œ ìƒì„±í•œ Data <span>$$G(x)$$</span>
- real_loss: ì‹¤ì œ Labelì„ 1ë¡œì„œ íŒë‹¨í•œë‹¤. <span>$$[log D_{Y}(y)]$$</span>
- generated_loss: G(x)ë¥¼ 0ë¡œì„œ íŒë‹¨í•œë‹¤. <span>$$\mathbb{E}_{x\text{~}p_{data}(x)}[1-log D_{Y}(G(x))]$$</span>

```python
LAMBDA = 10
loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real, generated):
    real_loss = loss_obj(tf.ones_like(real), real)

    generated_loss = loss_obj(tf.zeros_like(generated), generated)

    total_disc_loss = real_loss + generated_loss

    return total_disc_loss * 0.5
```
<br>
**generator_loss(generated)**
- generated: Generatorì—ì„œ ìƒì„±í•œ Data <span>$$G(x)$$</span>
- loss_obj(tf.ones_like(generated), generated): G(x)ë¥¼ 1ë¡œì„œ íŒë‹¨í•˜ê²Œ í•œë‹¤. <span>$$\mathbb{E}_{x\text{~}p_{data}(x)}[log D_{Y}(G(x))]$$</span>


```python
def generator_loss(generated):
    return loss_obj(tf.ones_like(generated), generated)
```
<br>
**calc_cycle_loss(real_image, cycled_image)**  
- real_image: ì‹¤ì œ ì›ë³¸ Image
- cycled_image: Reconsturcted Image <span>$$ğ¹(ğº(ğ‘¥))$$</span>
- tf.reduce_mean(tf.abs(real_image - cycled_image)): <span>$$||F(G(x))-x||_{1}$$</span>

```python
def calc_cycle_loss(real_image, cycled_image):
    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))
  
    return LAMBDA * loss1
```
<br>
**identity_loss(real_image, same_image)**  
- real_image: ì‹¤ì œ ì›ë³¸ Image
- same_image: Generate Image <span>$$G(ğ‘¥)$$</span>
- tf.reduce_mean(tf.abs(real_image - same_image)): <span>$$||G(x)-y||_{1}$$</span>

```python
def identity_loss(real_image, same_image):
    loss = tf.reduce_mean(tf.abs(real_image - same_image))
    return LAMBDA * 0.5 * loss
```
<br>
<br><br>

#### Define Optimizer
ê°ê°ì˜ Generator 2ê°œ, Discriminator2ê°œ ì´ 4ê°œì´ë¯€ë¡œ Optimizerë¥¼ 4ê°œ ì •ì˜í•œë‹¤.
```python
generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
```
<br>
<br><br>
#### CheckPoints
Modelì˜ ì¤‘ê°„ê³¼ì •ì„ ì €ì¥í•˜ê¸° ìœ„í•œ CheckPointsë¥¼ ì €ì¥í•œë‹¤.  
ìì„¸í•œ ì‚¬ìš©ë²•ì€ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì. <a href="https://www.tensorflow.org/guide/checkpoint">Checkpoint ì‚¬ìš©ë²•</a>
```python
checkpoint_path = "./checkpoints/train"

ckpt = tf.train.Checkpoint(generator_g=generator_g,
                           generator_f=generator_f,
                           discriminator_x=discriminator_x,
                           discriminator_y=discriminator_y,
                           generator_g_optimizer=generator_g_optimizer,
                           generator_f_optimizer=generator_f_optimizer,
                           discriminator_x_optimizer=discriminator_x_optimizer,
                           discriminator_y_optimizer=discriminator_y_optimizer)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

# if a checkpoint exists, restore the latest checkpoint.
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print ('Latest checkpoint restored!!')
```
<br>
<br><br>

#### Generate_image()
Trainingê³¼ì •ì—ì„œ Imageê°€ ì–´ë–»ê²Œ ë³€í™”ë˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•˜ì—¬ ì„ ì–¸í•œ Functionì´ë‹¤.  
í•˜ë‚˜ì˜ Image(X: Horse)ë¥¼ ì…ë ¥ë°›ì•„, Generator(<span>$$G: X -> Y$$</span>)ì˜ ê²°ê³¼(Y: Zebra)ë¥¼ í†µí•˜ì—¬ ì–´ë–»ê²Œ Generatorë˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤.
```python
EPOCHS = 40
def generate_images(model, test_input):
    prediction = model(test_input)
    
    plt.figure(figsize=(12, 12))

    display_list = [test_input[0], prediction[0]]
    title = ['Input Image', 'Predicted Image']

    for i in range(2):
        plt.subplot(1, 2, i+1)
        plt.title(title[i])
        # getting the pixel values between [0, 1] to plot it.
        plt.imshow(display_list[i] * 0.5 + 0.5)
        plt.axis('off')
    plt.show()
```
<br>
<br><br>
#### Trainstep
ì‹¤ì œ Modelì˜ Weightê°€ updateë˜ëŠ” ë¶€ë¶„ì„ ì§€ì •í•œë‹¤.  
ê°ê°ì˜ Loss Functionì€ ìœ„ì—ì„œ ì •í–ˆìœ¼ë¯€ë¡œ ê°ê°ì˜ Parameterê°€ ì–´ë–¤ê²ƒì„ ì˜ë¯¸í•˜ëŠ”ì§€ íŒŒì•…í•˜ë©´ ì‰½ê²Œ ëŒ€ì…í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- real_x(<span>$$x$$</span>): ì‹¤ì œ Training Image(Horse)
- real_y(<span>$$y$$</span>): ì‹¤ì œ Label Image(Zebra)
- fake_y(<span>$$G(x)$$</span>): Generate Image
- fake_x(<span>$$F(y)$$</span>): Generate Image
- cycled_x(<span>$$F(G(x))$$</span>): Reconstruction Image(â‰ƒHorse)
- cycled_y(<span>$$G(F(y))$$</span>): Reconstruction Image(â‰ƒZebra)
- same_x(<span>$$G(x)$$</span>): Generate Image
- same_y(<span>$$F(y)$$</span>): Generate Image

**í•˜ë‚˜ ì£¼ì˜í•´ì•¼ í•˜ëŠ” CodeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. <code>total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)</code>**  
ìœ„ì—ì„œ Generator LossFunctionì„ ì„¤ì •í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•˜ì˜€ë‹¤.  
<p>$$Loss_{G^{*} \text{  } Generator} = \underset{G}{max} \mathbb{E}_{x\text{~}p_{data}(x)}[log D_{Y}(G(x))] + \lambda_{1}L_{cyc}(G)+ \lambda_{2}L_{L1}(G)$$</p>
ë”°ë¼ì„œ GeneratorëŠ” í•´ë‹¹ë˜ëŠ” 3ê°œì˜ LossFunctionì˜ ê°’ì˜ í•©ì´ë¼ëŠ” ê²ƒì„ ê¸°ì–µí•´ì•¼ í•œë‹¤.

**ì°¸ì¡°**  
@tf.tufnctionì´ë‚˜ tf.GradientTape()ì— ëŒ€í•´ì„œ ëª¨ë¥´ì‹œëŠ” ë¶„ë“¤ì€ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•˜ë¯€ë¡œ ì°¸ì¡° ë§í¬ë¥¼ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤. TF2.0ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ Weightë¥¼ Updateí•©ë‹ˆë‹¤.
- <a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(6)/#eager-execution">EagerExecution</a>
- <a href="https://wjddyd66.github.io/tnesorflow2.0/Tensorflow2.0(10)/">tf.function</a>

```python
@tf.function
def train_step(real_x, real_y):
    # persistent is set to True because the tape is used more than
    # once to calculate the gradients.
    with tf.GradientTape(persistent=True) as tape:
        # Generator G translates X -> Y
        # Generator F translates Y -> X.
    
        fake_y = generator_g(real_x, training=True)
        cycled_x = generator_f(fake_y, training=True)

        fake_x = generator_f(real_y, training=True)
        cycled_y = generator_g(fake_x, training=True)

        # same_x and same_y are used for identity loss.
        same_x = generator_f(real_x, training=True)
        same_y = generator_g(real_y, training=True)

        disc_real_x = discriminator_x(real_x, training=True)
        disc_real_y = discriminator_y(real_y, training=True)

        disc_fake_x = discriminator_x(fake_x, training=True)
        disc_fake_y = discriminator_y(fake_y, training=True)

        # calculate the loss
        gen_g_loss = generator_loss(disc_fake_y)
        gen_f_loss = generator_loss(disc_fake_x)
    
        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)
    
        # Total generator loss = adversarial loss + cycle loss
        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)
        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)

        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)
        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)
  
    # Calculate the gradients for generator and discriminator
    generator_g_gradients = tape.gradient(total_gen_g_loss, 
                                        generator_g.trainable_variables)
    generator_f_gradients = tape.gradient(total_gen_f_loss, 
                                        generator_f.trainable_variables)
  
    discriminator_x_gradients = tape.gradient(disc_x_loss, 
                                            discriminator_x.trainable_variables)
    discriminator_y_gradients = tape.gradient(disc_y_loss, 
                                            discriminator_y.trainable_variables)
  
    # Apply the gradients to the optimizer
    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, 
                                            generator_g.trainable_variables))

    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, 
                                            generator_f.trainable_variables))
  
    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,
                                                discriminator_x.trainable_variables))
  
    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,
                                                discriminator_y.trainable_variables))
```
<br>
<br><br>
#### Training
ì§€ì •í•œ Epochìˆ˜ë§Œí¼ Trainingì„ ì‹¤ì‹œí•˜ë©´ì„œ Generator(<span>$$G: X -> Y$<span>)ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤.
```python
for epoch in range(EPOCHS):
    start = time.time()

    n = 0
    for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):
        train_step(image_x, image_y)
        if n % 10 == 0:
            print ('.', end='')
        n+=1

    clear_output(wait=True)
    # Using a consistent image (sample_horse) so that the progress of the model
    # is clearly visible.
    generate_images(generator_g, sample_horse)

    if (epoch + 1) % 5 == 0:
        ckpt_save_path = ckpt_manager.save()
        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,
                                                         ckpt_save_path))

    print ('Time taken for epoch {} is {} sec\n'.format(epoch + 1,
                                                      time.time()-start))
```
<br>
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/57.png" height="500" width="500" /></div><br>
<br><br>
#### Check TestSet
ì‹¤ì œ Trainingì„ ë§ˆì¹œ Generator(<span>$$G(X -> Y)$$</span>)ë¡œì„œ Testsetì˜ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤.
```python
# Run the trained model on the test dataset
for inp in test_horses.take(5):
    generate_images(generator_g, inp)
```
<br>

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/58.png" height="500" width="500" /></div><br>
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Tensorflow/59.png" height="500" width="500" /></div><br>
**ìœ„ì˜ Testsetê²°ê³¼ë¥¼ í™•ì¸í•˜ì—¬ ë³´ë©´ Trainingì‹œí‚¨ ê°ˆìƒ‰ë§ì„ ì¤‘ì ì ìœ¼ë¡œ Zebra í˜•íƒœë¡œ ë°”ë€ŒëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìœ¼ë‚˜, Pix2Pixë§Œí¼ Sharpí•˜ë©´ì„œ ëšœë ·í•œ ê²°ê³¼ë¥¼ ì–»ì§€ëŠ” ëª»í•˜ì˜€ë‹¤.**  

<hr>
ì°¸ì¡°: <a href="https://github.com/wjddyd66/Tensorflow2.0/blob/master/CycleGAN.ipynb">ì›ë³¸ì½”ë“œ</a><br>
ì°¸ì¡°: <a href="https://www.tensorflow.org/tutorials/generative/cyclegan?hl=ko">Tensorflow2.0 CycleGAN</a><br>
ì°¸ì¡°: <a href="https://mikigom.github.io/jekyll/update/2017/07/11/cyclegan.html">mikigom ë¸”ë¡œê·¸</a><br>
ì°¸ì¡°: <a href="https://taeoh-kim.github.io/blog/gan%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-image-to-image-translation-pix2pix-cyclegan-discogan/">taeoh-kim ë¸”ë¡œê·¸</a><br>

ì½”ë“œì— ë¬¸ì œê°€ ìˆê±°ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ wjddyd66@naver.comìœ¼ë¡œ  Mailì„ ë‚¨ê²¨ì£¼ì„¸ìš”.