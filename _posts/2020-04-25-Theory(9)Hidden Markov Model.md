---
layout: post
title:  "Theory9. Hidden Markov Model"
date:   2020-04-25 11:00:20 +0700
categories: [Machine Learning]
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 9. Hidden Markov Model
$$\newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}$$
$$\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}$$
ì´ë²ˆ PostëŠ” ë¬¸ì¼ì²  êµìˆ˜ë‹˜ì˜ ë¨¸ì‹ ëŸ¬ë‹ ë³´ë‹¤ëŠ” ì‹¤ì œ Problemì— ì ‘ëª©ì‹œì¼œì„œ Hidden Markov Modelì— ëŒ€í•˜ì—¬ ì•Œì•„ë³´ê³ , ì‹¤ì œ Modelì„ Codeë¡œì„œ í™•ì¸í•˜ëŠ” Postì…ë‹ˆë‹¤. (ë§ì€ ì±…ì—ì„œ CodeëŠ” ë‹¤ë£¨ì§€ ì•Šì•„ì„œ ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.)

- 9.1 What is Hidden Markov Model?
- 9.2 Viterbi Decoding Algorithm
- 9.3 Forward-Backward probability Cacluation
- 9.4 Baum-Welch Algorithm

### 9.1 What is Hidden Markov Model?
**HMM(Hidden Markov Model)ì´ë¼ëŠ” ê²ƒì€ Dataë¥¼ ê°€ì§€ê³  Hiddenì¸ Stateë¥¼ ì¸¡ì •í•˜ëŠ” Algorithmì´ë‹¤.**  

ì‹¤ì œ ì´ëŸ¬í•œ ì„¤ëª…ë§Œìœ¼ë¡œëŠ” ì™€ë‹¿ì§€ ì•Šìœ¼ë‹ˆ ì˜ˆì œë¡œ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
ì–´ë– í•œ Dataê°€ ATCGA... ê°™ì€ Dataê°€ ê´€ì¸¡ë˜ì—ˆë‹¤ê³  í•˜ì.  
ê°ê°ì˜ Data A or T or C or GëŠ” q0 or q1 or q2 or q3 or q4ì˜ Stateë¼ê³  ê°€ì •í•˜ì.  

ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” X(=ATCGA...)ë§Œ Observationí•˜ì—¬ì„œ Latent Varialbesì¸ Stateë¥¼ ì¸¡ì •í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.  

ì´ì „ Post <a href="">8. K-Means Clustering and Gaussian Mixture Model
</a>ì™€ ê°™ì´ Latent Variablesë¥¼ ì¸¡ì •í•´ì•¼ í•˜ë¯€ë¡œ EM Algorithmìœ¼ë¡œì„œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì€ StateëŠ” Intron, Exonì´ ì¡´ì¬í•˜ê²Œ ë˜ê³ , ê°ê°ì˜ Stateì˜ Emissionì€ A,T,C,Gê°€ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤.  

ì•ìœ¼ë¡œ ì‹¤ì œ ì ‘ê·¼í•  Exampleì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/25.png)

ìœ„ì˜ ë¬¸ì œì— ë§ê²Œ ì•ìœ¼ë¡œì˜ ì‹ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  Notationì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
- ğ´=<span>$$a_{ij}$$</span>: i ë²ˆì§¸ Stateì—ì„œ jë²ˆì§¸ Stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥  => Transition Probability
- ğµ=<span>$$b_i(o_t)$$</span>: i ë²ˆì§¸ Stateì—ì„œÂ ğ‘œğ‘¡ê°€ Emissionë  í™•ë¥  => Emission Probability
- ğ‘‚=[ğ´,ğ‘‡,ğ¶,ğº,ğ‘‡,ğ´]: ê´€ì¸¡ëœ Data -> Length: 6
- ğ‘„=[ğ‘0,ğ‘1,ğ‘2,ğ‘3,ğ‘4]: ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” Status

ëª¨ë“  í™•ë¥ ì€ Conditionalë¡œì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  
ì¦‰, ìƒê°í•´ë³´ë©´ ê°ê°ì˜ A,Bë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìƒê°í•  ìˆ˜ ìˆë‹¤.  

- <span>$$a_{ij} = P(j|i)$$</span>: í˜„ì¬ i Stateì¼ë•Œ ë‹¤ìŒ Stateê°€ jì¼ í™•ë¥ 
- <span>$$b_i(o_t) = P(o|i)$$</span>: í˜„ì¬ i Stateì¼ë•Œ oë¥¼ Emissioní•  í™•ë¥ 

ì¦‰, Conditional Probabilityë¡œì„œ ëª¨ë“  ê²ƒì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤.

### 9.2 Viterbi Decoding Algorithm
ë¨¼ì € ìµœì¢…ì ì¸ Viterbi Algorithmì„ ì‚´í´ë³´ë©´ Notationì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ ëœë‹¤.

- <span>$$ğ‘‰_t(ğ‘—)=max_i[ğ‘‰_tâˆ’1(ğ‘–)ğ‘_{ij}ğ‘_j]$$<span>: Viterbi Algorithm => të²ˆì§¸ ì‹œì ì—ì„œ jë²ˆì§¸ ì€ë‹‰ ìƒíƒœê°€ ê´€ì¸¡ë˜ê³  ê´€ì¸¡ì¹˜Â ğ‘‚ğ‘¡(=A or T or C or G) ê°€ ê´€ì¸¡ë  í™•ë¥ 
 - j=0: A
 - j=1: T
 - j=2: C
 - j=3: G
- <span>$$b_t(j)= \argmax_i[V_{tâˆ’1}(ğ‘–)âˆ—a_{ij}âˆ—b_j(o_t)]$$</span>: Traceback => í™•ë¥ ì´ ë†’ì€ Statusë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ Traceback
  

í˜„ì¬ ì‹¤ì œ DataëŠ” ATCGTAê°€ ê´€ì¸¡ë˜ì—ˆë‹¤. ê°ê°ì˜ Stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì´ë‘, ê°ê°ì˜ Stateì—ì„œ Emissionë  í™•ë¥ ì´ ì¡´ì¬í•˜ë¯€ë¡œ, ì´ëŸ¬í•œ Sequenceê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²½ë¡œë¥¼ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.(ê°ˆ ìˆ˜ ì—†ëŠ” ê³³ì€ ì œì™¸í•œë‹¤.)  

Viterbi Algorithmê°’ì„ ìƒê°í•´ë³´ë©´, i->jê°€ ë ìˆ˜ ìˆëŠ” ëª¨ë“  Transmission Probabilityì™€ ië²ˆì§¸ì˜ ê°ê°ì˜ Stateì—ì„œ Emissionë  Probabilityì˜ ê³± ì¤‘ ê°€ì¥ í° ê°’ì„ ì„ íƒí•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ê°€ì¥ ë†’ì„ í™•ë¥ ì„ ì„ íƒí•˜ê²Œ ë˜ë©´, Data Sequenceì— ë§ëŠ” í™•ë¥ ì´ ë†’ì€ Stateë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆë‹¤.  
    
Tracebackì„ ì‚´í´ë³´ê²Œ ë˜ë©´, Viterbi Algorithmì€ MAXê°’ì„ ì„ íƒí•˜ë¯€ë¡œ ê·¸ ê°’ì„ ì–´ë””ì—ë‹¤ê°€ ì €ì¥í•´ë‘ë©´, Argmaxë¥¼ í†µí•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê³³ìœ¼ë¡œì„œ Tracebackì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒ ì´ë‹¤.


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/26.png)
    
ìœ„ì˜ ê·¸ë¦¼ì„ Matrixë¡œì„œ í‘œí˜„í•˜ê¸° ìœ„í•˜ì—¬ ê°ê°ì˜Â ğ‘‰ğ‘–(ğ‘—)ë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Viterbi Algorithmì‹ì€ Maxë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•˜ë‚˜, q2ë¥¼ ì˜ˆì‹œë¡œ í•˜ë©´, q1 -> q2ëŠ” ì²˜ìŒë§Œ ê°€ëŠ¥í•˜ê³ , q1 -> q2, q3 -> q2, q4 -> q2ëŠ” ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ q3ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ maxë¡œì„œ ê°’ì„ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ê²½ìš°ì˜ ìˆ˜ê°€ í•˜ë‚˜ë§Œ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ì‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.)

ğ‘‰1(1) = ğ‘01âˆ—ğ‘0(0) =1âˆ—0.1=0.1

ğ‘‰2(2)=ğ‘‰1(1)âˆ—ğ‘12âˆ—ğ‘2(1)=0.1âˆ—0.5âˆ—0.25=0.0125
ğ‘‰2(3)=ğ‘‰1(1)âˆ—ğ‘13âˆ—ğ‘3(1)=0.1âˆ—0.5âˆ—0.17=0.0085

ğ‘‰3(2)=ğ‘‰2(2)âˆ—ğ‘22âˆ—ğ‘2(2)=0.0125âˆ—0.65âˆ—0.15=0.00121875
ğ‘‰3(3)=ğ‘‰2(3)âˆ—ğ‘33âˆ—ğ‘3(2)=0.0085âˆ—0.8âˆ—0.43=0.002924
ğ‘‰3(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰2(2)âˆ—ğ‘24âˆ—ğ‘4(2),ğ‘‰2(3)âˆ—ğ‘34âˆ—ğ‘4(2),0]
=ğ‘šğ‘ğ‘¥[0,0.0009625,0.000374,0]=0.0009625
ğ‘‰4(2)=ğ‘‰3(2)âˆ—ğ‘22âˆ—ğ‘(3)=0.00121875âˆ—0.65âˆ—0.25=0.000198047
ğ‘‰4(3)=ğ‘‰3(3)âˆ—ğ‘33âˆ—ğ‘3(3)=0.002924âˆ—0.8âˆ—0.29=0.000678368
ğ‘‰4(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰3(2)âˆ—ğ‘24âˆ—ğ‘4(3),ğ‘‰3(3)âˆ—ğ‘34âˆ—ğ‘4(3),0]
=ğ‘šğ‘ğ‘¥[0,0.000157828,0.000216376,0]=0.000216376

ğ‘‰5(2)=ğ‘‰4(2)âˆ—ğ‘22âˆ—ğ‘2(1)=0.000198047âˆ—0.65âˆ—0.25=0.000032183
ğ‘‰5(3)=ğ‘‰4(3)âˆ—ğ‘33âˆ—ğ‘3(1)=0.000678368âˆ—0.8âˆ—0.17=0.000092258
ğ‘‰5(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰4(2)âˆ—ğ‘24âˆ—ğ‘4(1),ğ‘‰4(3)âˆ—ğ‘34âˆ—ğ‘4(1),0]
=ğ‘šğ‘ğ‘¥[0,0.00009704,0.000018997,0]=0.000018997

ğ‘‰6(2)=ğ‘‰5(2)âˆ—ğ‘22âˆ—ğ‘2(0)=0.000032183âˆ—0.65âˆ—0.35=0.000007322
ğ‘‰6(3)=ğ‘‰5(3)âˆ—ğ‘33âˆ—ğ‘3(0)=0.000092258âˆ—0.8âˆ—0.11=0.000008119
ğ‘‰6(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰5(2)âˆ—ğ‘24âˆ—ğ‘4(0),ğ‘‰5(3)âˆ—ğ‘34âˆ—ğ‘4(0),0]
=ğ‘šğ‘ğ‘¥[0,0.000003481,0.000004982,0]=0.000004982

ì‹¤ì œ ê³„ì‚°í•œ ê°’ì„ Matrixë¡œì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
    
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/27.png)
    
Trace Backì„ ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
- End:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0,0,1âˆ—.000004982]=3=ğ‘4
- End-1: ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0.000003481,0.000004982,0]=q2
- End-2:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0,0.000092258,0]=2=ğ‘3

    ...
    
- Start + 1:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0.1,0,0,0]=0=ğ‘1

ë”°ë¼ì„œ Tracebackì˜ ê²°ê³¼ë¡œ ì¸í•˜ì—¬ Stateê°€ ë³€í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
ğ‘0â†’ğ‘1â†’ğ‘3â†’ğ‘3â†’ğ‘3â†’ğ‘3â†’ğ‘4â†’ğ‘0  
    
ìœ„ì˜ ê³¼ì •ì„ Matrixì— ì—°ê´€ì§€ì–´ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•  ìˆ˜ ìˆë‹¤.
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/28.png)

### 9.3 Forward-Backward probability Cacluation
**Hidden Markov Modelì˜ ì „ë°˜ì ì¸ ë‚´ìš©ê³¼ Stateë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆëŠ” Viterbi Algorithmì˜ ê²½ìš°ì—ëŠ” ê°„ë‹¨í•˜ë¯€ë¡œ ì‹¤ì œ Dataì— ì ìš©ì„ í•˜ì—¬ ì•Œì•„ë³´ì•˜ë‹¤.**  

Forward-Backwardì™€ Baum-Welch Algorithmì˜ ê²½ìš°ì—ëŠ” Modelì„ ì‹¤ì§ˆì ìœ¼ë¡œ Trainningí•˜ëŠ” ë¶€ë¶„ì´ë¯€ë¡œ ì¢€ ë” Genearlí•œ ìƒíƒœì˜ ìˆ˜ì‹ì„ ìœ ë„í•´ê°€ë©° ì•Œì•„ë³´ì. (ì´ì „ì— ì‚¬ìš©í•œ Notationì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.)  

ê°ê°ì˜ Forward, Backward ProbabilityëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•©ë‹ˆë‹¤.
- Forward Probability: <span>$$\alpha_t(j) = \sum_{i=1}^{n} \alpha_{t-1}a_{ij}b_j(o_t)$$</span>
- Backward Probability: <span>$$\beta_t(i) = \sum_{i=1}^{n} \beta_{t+1}(j)a_{ij}b_j(o_t)$$</span>

**Viterbi Algorithmì˜ ì‹ì¸ <span>$$ğ‘‰_t(ğ‘—)=max_i[ğ‘‰_tâˆ’1(ğ‘–)ğ‘_{ij}ğ‘_j]$$</span>ì™€ ë¹„êµí•˜ê²Œ ë˜ë©´, Viterbi Algorithmì€ Maxê°’ì„ ì°¾ìœ¼ë¯€ë¡œ Indexingì„ í†µí•˜ì—¬ TraceBackì´ ê°€ëŠ¥í•˜ì˜€ë‹¤ë©´, Forward-Backward ProbabilityëŠ” ëª¨ë“  í™•ë¥ ì„ Summationí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— TraceBackì´ ë¶ˆê°€ëŠ¥ í•˜ë‹¤. í•˜ì§€ë§Œ, Summationì´ë¯€ë¡œ ì´ë¥¼ í™œìš©í•˜ì—¬ ê°ê°ì˜ í™•ë¥ ì— ëŒ€í•˜ì—¬ Updateê°€ ê°€ëŠ¥í•˜ë‹¤.**  

ìµœì¢…ì ìœ¼ë¡œ Modelì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´(9-5 Code) Forward-Backward Probabilityë¥¼ ì‚¬ìš©í•œ Baum-Welch Algorithmìœ¼ë¡œì„œ Updateë¥¼ í•˜ê²Œ ë˜ê³ , Viterbi Algorithmìœ¼ë¡œì„œ Modelì„ í‰ê°€í•˜ê²Œ ëœë‹¤.

ì „ë°©í™•ë¥ (Forward Probability)ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
<img src="https://i.imgur.com/mbBaTch.png"><br>
ì‚¬ì§„ ì°¸ì¡°: <a href="https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/">ratsgo ë¸”ë¡œê·¸</a><br>
<p>$${ \alpha  }_{ 3 }(4)=\sum _{ i=1 }^{ 4 }{ { \alpha  }_{ 2 }(i)\times { a }_{ i4 } } \times { b }_{ 4 }({ o }_{ 3 })$$</p>

í›„ë°©í™•ë¥ (Backward Probability)ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
<img src="https://i.imgur.com/bP9BdJy.png"><br>
ì‚¬ì§„ ì°¸ì¡°: <a href="https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/">ratsgo ë¸”ë¡œê·¸</a><br>
<p>$${ \beta  }_{ 3 }(4)=\sum _{ j=1 }^{ 4 }{ { a }_{ 4j } } \times { b }_{ j }({ o }_{ 4 })\times { \beta  }_{ 4 }(j)$$</p>

**ì´ ë‘í™•ë¥ ì„ ê³±í•˜ë©´ íŠ¹ì • Nodeë¥¼ ì§€ë‚˜ëŠ” ëª¨ë“  Probabilityë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.**  

ì‚¬ì§„ìœ¼ë¡œì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
<img src="https://i.imgur.com/3SQDk3b.png"><br>
ì‚¬ì§„ ì°¸ì¡°: <a href="https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/">ratsgo ë¸”ë¡œê·¸</a><br>
<p>$${ \alpha  }_{ t }\left( j \right) \times { \beta  }_{ t }\left( j \right) =P\left( { q }_{ t }=j,O|\theta  \right)$$</p>

ìœ„ì˜ ìˆ˜ì‹ì„ í™œìš©í•˜ë©´ HMMì˜ ëª¨ë“  í™•ë¥ ì— ëŒ€í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.(Start StateëŠ” q0ë¼ê³  ìƒê°í•œë‹¤ë©´)  
<p>$$P(O|\theta) = \sum_{i=1}^{n} \alpha_t(s)\beta_t(s) = P(q_t=q_0,O | \theta) = \beta_o(q_0)$$</p>

### 9.4 Baum-Welch Algorithm
ìš°ë¦¬ëŠ” ìœ„ì—ì„œ <span>$${ \alpha  }_{ t }\left( j \right) \times { \beta  }_{ t }\left( j \right) =P\left( { q }_{ t }=j,O|\theta  \right)$$</span>ì‹ì„ ì–»ì—ˆë‹¤.  

ì˜ ìƒê°í•´ë³´ë…€ Baum-Welch Algorithmì€ EM Algorithmì´ë‹¤.  
Latente Variableì¸ Stateë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•˜ì—¬ Forward, Backward ê°’ì„ ê³„ì‚°í•˜ëŠ” ë‹¨ê³„ê°€ E-Stepì´ê³  ì´ëŸ¬í•œ ê°’ì„ í™œìš©í•˜ì—¬ A,B,Initial Probabilityë¥¼ Updateí•˜ê¸° ë•Œë¬¸ì´ë‹¤.  

**M-Step**  
**1. Emission Probability**  
íŠ¹ì • tì‹œì ì—ì„œ Observationì´ jì¼ í™•ë¥ ì€ ë§¤ìš° ê³„ì‚°í•˜ê¸° ì‰½ë‹¤.  
<p>$$\gamma_t(j) = P\left( { q }_{ t }=j|O,\theta  \right)$$</p>
<p>$$= \frac{P\left( { q }_{ t }=j,O|\theta  \right)}{P(O|\theta)} = \frac{{ \alpha  }_{ t }\left( j \right) \times { \beta  }_{ t }\left( j \right)}{\sum_{i=1}^{n} \alpha_t(s)\beta_t(s)}$$</p>
ìœ„ì—ì„œ ë¯¸ë¦¬ êµ¬í•œ ì‹ìœ¼ë¡œì„œ í¸í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.  

ìœ„ì˜ ì‹ì„ í™œìš©í•˜ì—¬ ì‹¤ì œ Emission Probabilityë¥¼ Updateí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  

<p>$$\hat{b_j}(v_k) = \frac{\sum_{t=1, s.t.o_t=v_k}^{T}\gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}$$</p>

**ìœ„ì˜ ì‹ì„ ì‚´í´ë³´ê²Œ ë˜ë©´, ëª¨ë“  Observationì—ì„œ Emission Probabilityë¥¼ ê³„ì‚°í•œ ê°’ê³¼ Modelì´ ì˜ˆì¸¡í•œ Observationì´ ì‹¤ì œ Observationì´ ê°™ì€ ë•Œì˜ í™•ë¥ ë¡œì„œ ë‚˜íƒ€ë‚´ê²Œ ëœë‹¤.**  

**2. Transmission Probability**  
Transmissionì¸ ê²½ìš°ì—ëŠ” í•œê°€ì§€ ë” ìƒê°í•´ì•¼ í•˜ëŠ” ì ì´ ìˆë‹¤. íŠ¹ì • ì‹œì ì—ì„œì˜ Emissionì´ <span>${ q }_{ t }=i$</span>ì¸ ê²½ìš°ì— <span>${ q }_{ t +1}=j$</span>ì´ ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—ë‹¤. ì¦‰, í˜„ì¬ ì¸¡ì •í•˜ê³ ìí•˜ëŠ” tì‹œì ì—ì„œ ë‹¤ìŒ ì‹œì ê¹Œì§€ ìƒê°í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.  

ë”°ë¼ì„œ ìœ„ì—ì„œ êµ¬í•œ ì‹ì—ì„œ <span>$$a_{ij}b_{j}(o_t)$$</span>ë¥¼ ê³±í•´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒ ì´ë‹¤. ì´ë¥¼ ì‹ìœ¼ë¡œì„œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
<p>$$% <![CDATA[
\begin{align*}
{ \xi  }_{ t }\left( i,j \right) =&\frac { P\left( { q }_{ t }=i,{ q }_{ t+1 }=j,O|\lambda  \right)  }{ P\left( O|\lambda  \right)  } \\ =&\frac { { \alpha  }_{ t }\times { a }_{ ij }\times { b }_{ j }\left( { o }_{ t+1 } \right) \times { \beta  }_{ t+1 }\left( j \right)  }{ \sum _{ s=1 }^{ n }{ \alpha _{ t }\left( s \right) \times \beta _{ t }\left( s \right)  }  }
\end{align*} %]]>$$</p>

ìœ„ì˜ ìˆ˜ì‹ì„ í™œìš©í•˜ì—¬ Transmission Probabilityë¥¼ Updateí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  
<p>$$\hat { a } _{ ij }=\frac { \sum _{ t=1 }^{ T-1 }{ { \xi  }_{ t }\left( i,j \right)  }  }{ \sum _{ t=1 }^{ T-1 }{ \sum _{ k=1 }^{ N }{ { \xi  }_{ t }\left( i,k \right)  }  }  }$$</p>

**ìœ„ì˜ ì‹ì„ ì‚´í´ë³´ê²Œ ë˜ë©´ iì‹œì ì—ì„œ jë²ˆì§¸ ì‹œì ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆëŠ” ëª¨ë“  Transimission Probabilityì—ì„œ ì‹¤ì œ í™•ë¥ ë¡œì„œ ê°„ Transmission Probabilityì˜ í™•ë¥ ë¡œì„œ ë‚˜íƒ€ë‚¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.**  

ìœ„ì˜ ë‘ê°€ì§€ì— ëŒ€í•œ ëª¨ë“  ìì„¸í•œ ìˆ˜ì‹ì€ <a href="https://wjddyd66.github.io/machine%20learning/Theory(8)K-Means-Clustering-and-Gaussian-Mixture-Model(3)/">EM-Algorithm</a>ì„ ì‚¬ìš©í•˜ê³ , <a href="https://kooc.kaist.ac.kr/machinelearning2__17/lecture/10872/">ë¬¸ì¼ì²  êµìˆ˜ë‹˜ ê°•ì˜</a>ì—ì„œ ìì„¸í•œ ìœ ë„ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
