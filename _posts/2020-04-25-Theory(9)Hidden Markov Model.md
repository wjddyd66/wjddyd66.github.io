---
layout: post
title:  "Theory9. Hidden Markov Model"
date:   2020-04-25 11:00:20 +0700
categories: [Machine Learning]
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 9. Hidden Markov Model
$$\newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}$$
$$\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}$$
ì´ë²ˆ PostëŠ” ë¬¸ì¼ì²  êµìˆ˜ë‹˜ì˜ ë¨¸ì‹ ëŸ¬ë‹ ë³´ë‹¤ëŠ” ì‹¤ì œ Problemì— ì ‘ëª©ì‹œì¼œì„œ Hidden Markov Modelì— ëŒ€í•˜ì—¬ ì•Œì•„ë³´ê³ , ì‹¤ì œ Modelì„ Codeë¡œì„œ í™•ì¸í•˜ëŠ” Postì…ë‹ˆë‹¤. (ë§ì€ ì±…ì—ì„œ CodeëŠ” ë‹¤ë£¨ì§€ ì•Šì•„ì„œ ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.)

- 9.1 What is Hidden Markov Model?
- 9.2 Viterbi Decoding Algorithm
- 9.3 Forward-Backward probability Cacluation
- 9.4 Baum-Welch Algorithm
- 9.5 Hidden Markov Code

### 9.1 What is Hidden Markov Model?
**HMM(Hidden Markov Model)ì´ë¼ëŠ” ê²ƒì€ Dataë¥¼ ê°€ì§€ê³  Hiddenì¸ Stateë¥¼ ì¸¡ì •í•˜ëŠ” Algorithmì´ë‹¤.**  

ì‹¤ì œ ì´ëŸ¬í•œ ì„¤ëª…ë§Œìœ¼ë¡œëŠ” ì™€ë‹¿ì§€ ì•Šìœ¼ë‹ˆ ì˜ˆì œë¡œ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
ì–´ë– í•œ Dataê°€ ATCGA... ê°™ì€ Dataê°€ ê´€ì¸¡ë˜ì—ˆë‹¤ê³  í•˜ì.  
ê°ê°ì˜ Data A or T or C or GëŠ” q0 or q1 or q2 or q3 or q4ì˜ Stateë¼ê³  ê°€ì •í•˜ì.  

ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” X(=ATCGA...)ë§Œ Observationí•˜ì—¬ì„œ Latent Varialbesì¸ Stateë¥¼ ì¸¡ì •í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.  

ì´ì „ Post <a href="">8. K-Means Clustering and Gaussian Mixture Model
</a>ì™€ ê°™ì´ Latent Variablesë¥¼ ì¸¡ì •í•´ì•¼ í•˜ë¯€ë¡œ EM Algorithmìœ¼ë¡œì„œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì€ StateëŠ” Intron, Exonì´ ì¡´ì¬í•˜ê²Œ ë˜ê³ , ê°ê°ì˜ Stateì˜ Emissionì€ A,T,C,Gê°€ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤.  

ì•ìœ¼ë¡œ ì‹¤ì œ ì ‘ê·¼í•  Exampleì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/25.png)

ìœ„ì˜ ë¬¸ì œì— ë§ê²Œ ì•ìœ¼ë¡œì˜ ì‹ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  Notationì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
- ğ´=<span>$$a_{ij}$$</span>: i ë²ˆì§¸ Stateì—ì„œ jë²ˆì§¸ Stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥  => Transition Probability
- ğµ=<span>$$b_i(o_t)$$</span>: i ë²ˆì§¸ Stateì—ì„œÂ ğ‘œğ‘¡ê°€ Emissionë  í™•ë¥  => Emission Probability
- ğ‘‚=[ğ´,ğ‘‡,ğ¶,ğº,ğ‘‡,ğ´]: ê´€ì¸¡ëœ Data -> Length: 6
- ğ‘„=[ğ‘0,ğ‘1,ğ‘2,ğ‘3,ğ‘4]: ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” Status

ëª¨ë“  í™•ë¥ ì€ Conditionalë¡œì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  
ì¦‰, ìƒê°í•´ë³´ë©´ ê°ê°ì˜ A,Bë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìƒê°í•  ìˆ˜ ìˆë‹¤.  

- <span>$$a_{ij} = P(j|i)$$</span>: í˜„ì¬ i Stateì¼ë•Œ ë‹¤ìŒ Stateê°€ jì¼ í™•ë¥ 
- <span>$$b_i(o_t) = P(o|i)$$</span>: í˜„ì¬ i Stateì¼ë•Œ oë¥¼ Emissioní•  í™•ë¥ 

ì¦‰, Conditional Probabilityë¡œì„œ ëª¨ë“  ê²ƒì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤.

### 9.2 Viterbi Decoding Algorithm
ë¨¼ì € ìµœì¢…ì ì¸ Viterbi Algorithmì„ ì‚´í´ë³´ë©´ Notationì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ ëœë‹¤.

- <span>$$ğ‘‰_t(ğ‘—)=max_i[ğ‘‰_tâˆ’1(ğ‘–)ğ‘_{ij}ğ‘_j]$$<span>: Viterbi Algorithm => të²ˆì§¸ ì‹œì ì—ì„œ jë²ˆì§¸ ì€ë‹‰ ìƒíƒœê°€ ê´€ì¸¡ë˜ê³  ê´€ì¸¡ì¹˜Â ğ‘‚ğ‘¡(=A or T or C or G) ê°€ ê´€ì¸¡ë  í™•ë¥ 
 - j=0: A
 - j=1: T
 - j=2: C
 - j=3: G
- <span>$$b_t(j)= \argmax_i[V_{tâˆ’1}(ğ‘–)âˆ—a_{ij}âˆ—b_j(o_t)]$$</span>: Traceback => í™•ë¥ ì´ ë†’ì€ Statusë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ Traceback
  

í˜„ì¬ ì‹¤ì œ DataëŠ” ATCGTAê°€ ê´€ì¸¡ë˜ì—ˆë‹¤. ê°ê°ì˜ Stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì´ë‘, ê°ê°ì˜ Stateì—ì„œ Emissionë  í™•ë¥ ì´ ì¡´ì¬í•˜ë¯€ë¡œ, ì´ëŸ¬í•œ Sequenceê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²½ë¡œë¥¼ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.(ê°ˆ ìˆ˜ ì—†ëŠ” ê³³ì€ ì œì™¸í•œë‹¤.)  

Viterbi Algorithmê°’ì„ ìƒê°í•´ë³´ë©´, i->jê°€ ë ìˆ˜ ìˆëŠ” ëª¨ë“  Transmission Probabilityì™€ ië²ˆì§¸ì˜ ê°ê°ì˜ Stateì—ì„œ Emissionë  Probabilityì˜ ê³± ì¤‘ ê°€ì¥ í° ê°’ì„ ì„ íƒí•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ê°€ì¥ ë†’ì„ í™•ë¥ ì„ ì„ íƒí•˜ê²Œ ë˜ë©´, Data Sequenceì— ë§ëŠ” í™•ë¥ ì´ ë†’ì€ Stateë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆë‹¤.  
    
Tracebackì„ ì‚´í´ë³´ê²Œ ë˜ë©´, Viterbi Algorithmì€ MAXê°’ì„ ì„ íƒí•˜ë¯€ë¡œ ê·¸ ê°’ì„ ì–´ë””ì—ë‹¤ê°€ ì €ì¥í•´ë‘ë©´, Argmaxë¥¼ í†µí•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê³³ìœ¼ë¡œì„œ Tracebackì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒ ì´ë‹¤.


![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/26.png)
    
ìœ„ì˜ ê·¸ë¦¼ì„ Matrixë¡œì„œ í‘œí˜„í•˜ê¸° ìœ„í•˜ì—¬ ê°ê°ì˜Â ğ‘‰ğ‘–(ğ‘—)ë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Viterbi Algorithmì‹ì€ Maxë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•˜ë‚˜, q2ë¥¼ ì˜ˆì‹œë¡œ í•˜ë©´, q1 -> q2ëŠ” ì²˜ìŒë§Œ ê°€ëŠ¥í•˜ê³ , q1 -> q2, q3 -> q2, q4 -> q2ëŠ” ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ q3ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ maxë¡œì„œ ê°’ì„ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ê²½ìš°ì˜ ìˆ˜ê°€ í•˜ë‚˜ë§Œ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ì‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.)

ğ‘‰1(1) = ğ‘01âˆ—ğ‘0(0) =1âˆ—0.1=0.1

ğ‘‰2(2)=ğ‘‰1(1)âˆ—ğ‘12âˆ—ğ‘2(1)=0.1âˆ—0.5âˆ—0.25=0.0125
ğ‘‰2(3)=ğ‘‰1(1)âˆ—ğ‘13âˆ—ğ‘3(1)=0.1âˆ—0.5âˆ—0.17=0.0085

ğ‘‰3(2)=ğ‘‰2(2)âˆ—ğ‘22âˆ—ğ‘2(2)=0.0125âˆ—0.65âˆ—0.15=0.00121875
ğ‘‰3(3)=ğ‘‰2(3)âˆ—ğ‘33âˆ—ğ‘3(2)=0.0085âˆ—0.8âˆ—0.43=0.002924
ğ‘‰3(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰2(2)âˆ—ğ‘24âˆ—ğ‘4(2),ğ‘‰2(3)âˆ—ğ‘34âˆ—ğ‘4(2),0]=ğ‘šğ‘ğ‘¥[0,0.0009625,0.000374,0]=0.0009625
ğ‘‰4(2)=ğ‘‰3(2)âˆ—ğ‘22âˆ—ğ‘(3)=0.00121875âˆ—0.65âˆ—0.25=0.000198047
ğ‘‰4(3)=ğ‘‰3(3)âˆ—ğ‘33âˆ—ğ‘3(3)=0.002924âˆ—0.8âˆ—0.29=0.000678368
ğ‘‰4(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰3(2)âˆ—ğ‘24âˆ—ğ‘4(3),ğ‘‰3(3)âˆ—ğ‘34âˆ—ğ‘4(3),0]=ğ‘šğ‘ğ‘¥[0,0.000157828,0.000216376,0]=0.000216376

ğ‘‰5(2)=ğ‘‰4(2)âˆ—ğ‘22âˆ—ğ‘2(1)=0.000198047âˆ—0.65âˆ—0.25=0.000032183
ğ‘‰5(3)=ğ‘‰4(3)âˆ—ğ‘33âˆ—ğ‘3(1)=0.000678368âˆ—0.8âˆ—0.17=0.000092258
ğ‘‰5(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰4(2)âˆ—ğ‘24âˆ—ğ‘4(1),ğ‘‰4(3)âˆ—ğ‘34âˆ—ğ‘4(1),0]=ğ‘šğ‘ğ‘¥[0,0.00009704,0.000018997,0]=0.000018997

ğ‘‰6(2)=ğ‘‰5(2)âˆ—ğ‘22âˆ—ğ‘2(0)=0.000032183âˆ—0.65âˆ—0.35=0.000007322
ğ‘‰6(3)=ğ‘‰5(3)âˆ—ğ‘33âˆ—ğ‘3(0)=0.000092258âˆ—0.8âˆ—0.11=0.000008119
ğ‘‰6(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰5(2)âˆ—ğ‘24âˆ—ğ‘4(0),ğ‘‰5(3)âˆ—ğ‘34âˆ—ğ‘4(0),0]=ğ‘šğ‘ğ‘¥[0,0.000003481,0.000004982,0]=0.000004982

ì‹¤ì œ ê³„ì‚°í•œ ê°’ì„ Matrixë¡œì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
    
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/27.png)
    
Trace Backì„ ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
- End:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0,0,1âˆ—.000004982]=3=ğ‘4
- End-1: ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0.000003481,0.000004982,0]=q2
- End-2:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0,0.000092258,0]=2=ğ‘3

    ...
    
- Start + 1:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0.1,0,0,0]=0=ğ‘1

ë”°ë¼ì„œ Tracebackì˜ ê²°ê³¼ë¡œ ì¸í•˜ì—¬ Stateê°€ ë³€í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
ğ‘0â†’ğ‘1â†’ğ‘3â†’ğ‘3â†’ğ‘3â†’ğ‘3â†’ğ‘4â†’ğ‘0  
    
ìœ„ì˜ ê³¼ì •ì„ Matrixì— ì—°ê´€ì§€ì–´ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•  ìˆ˜ ìˆë‹¤.
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/28.png)

### 9.3 Forward-Backward probability Cacluation
**Hidden Markov Modelì˜ ì „ë°˜ì ì¸ ë‚´ìš©ê³¼ Stateë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆëŠ” Viterbi Algorithmì˜ ê²½ìš°ì—ëŠ” ê°„ë‹¨í•˜ë¯€ë¡œ ì‹¤ì œ Dataì— ì ìš©ì„ í•˜ì—¬ ì•Œì•„ë³´ì•˜ë‹¤.**  

Forward-Backwardì™€ Baum-Welch Algorithmì˜ ê²½ìš°ì—ëŠ” Modelì„ ì‹¤ì§ˆì ìœ¼ë¡œ Trainningí•˜ëŠ” ë¶€ë¶„ì´ë¯€ë¡œ ì¢€ ë” Genearlí•œ ìƒíƒœì˜ ìˆ˜ì‹ì„ ìœ ë„í•´ê°€ë©° ì•Œì•„ë³´ì. (ì´ì „ì— ì‚¬ìš©í•œ Notationì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.)  

ê°ê°ì˜ Forward, Backward ProbabilityëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•©ë‹ˆë‹¤.
- Forward Probability: <span>$$\alpha_t(j) = \sum_{i=1}^{n} \alpha_{t-1}a_{ij}b_j(o_t)$$</span>
- Backward Probability: <span>$$\beta_t(i) = \sum_{i=1}^{n} \beta_{t+1}(j)a_{ij}b_j(o_t)$$</span>

**Viterbi Algorithmì˜ ì‹ì¸ <span>$$ğ‘‰_t(ğ‘—)=max_i[ğ‘‰_tâˆ’1(ğ‘–)ğ‘_{ij}ğ‘_j]$$</span>ì™€ ë¹„êµí•˜ê²Œ ë˜ë©´, Viterbi Algorithmì€ Maxê°’ì„ ì°¾ìœ¼ë¯€ë¡œ Indexingì„ í†µí•˜ì—¬ TraceBackì´ ê°€ëŠ¥í•˜ì˜€ë‹¤ë©´, Forward-Backward ProbabilityëŠ” ëª¨ë“  í™•ë¥ ì„ Summationí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— TraceBackì´ ë¶ˆê°€ëŠ¥ í•˜ë‹¤. í•˜ì§€ë§Œ, Summationì´ë¯€ë¡œ ì´ë¥¼ í™œìš©í•˜ì—¬ ê°ê°ì˜ í™•ë¥ ì— ëŒ€í•˜ì—¬ Updateê°€ ê°€ëŠ¥í•˜ë‹¤.**  

ìµœì¢…ì ìœ¼ë¡œ Modelì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´(9-5 Code) Forward-Backward Probabilityë¥¼ ì‚¬ìš©í•œ Baum-Welch Algorithmìœ¼ë¡œì„œ Updateë¥¼ í•˜ê²Œ ë˜ê³ , Viterbi Algorithmìœ¼ë¡œì„œ Modelì„ í‰ê°€í•˜ê²Œ ëœë‹¤.

ì „ë°©í™•ë¥ (Forward Probability)ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
<img src="https://i.imgur.com/mbBaTch.png"><br>
ì‚¬ì§„ ì°¸ì¡°: <a href="https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/">ratsgo ë¸”ë¡œê·¸</a><br>
<p>$${ \alpha  }_{ 3 }(4)=\sum _{ i=1 }^{ 4 }{ { \alpha  }_{ 2 }(i)\times { a }_{ i4 } } \times { b }_{ 4 }({ o }_{ 3 })$$</p>

í›„ë°©í™•ë¥ (Backward Probability)ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
<img src="https://i.imgur.com/bP9BdJy.png"><br>
ì‚¬ì§„ ì°¸ì¡°: <a href="https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/">ratsgo ë¸”ë¡œê·¸</a><br>
<p>$${ \beta  }_{ 3 }(4)=\sum _{ j=1 }^{ 4 }{ { a }_{ 4j } } \times { b }_{ j }({ o }_{ 4 })\times { \beta  }_{ 4 }(j)$$</p>

**ì´ ë‘í™•ë¥ ì„ ê³±í•˜ë©´ íŠ¹ì • Nodeë¥¼ ì§€ë‚˜ëŠ” ëª¨ë“  Probabilityë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.**  

ì‚¬ì§„ìœ¼ë¡œì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
<img src="https://i.imgur.com/3SQDk3b.png"><br>
ì‚¬ì§„ ì°¸ì¡°: <a href="https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/">ratsgo ë¸”ë¡œê·¸</a><br>
<p>$${ \alpha  }_{ t }\left( j \right) \times { \beta  }_{ t }\left( j \right) =P\left( { q }_{ t }=j,O|\theta  \right)$$</p>

ìœ„ì˜ ìˆ˜ì‹ì„ í™œìš©í•˜ë©´ HMMì˜ ëª¨ë“  í™•ë¥ ì— ëŒ€í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.(Start StateëŠ” q0ë¼ê³  ìƒê°í•œë‹¤ë©´)  
<p>$$P(O|\theta) = \sum_{i=1}^{n} \alpha_t(s)\beta_t(s) = P(q_t=q_0,O | \theta) = \beta_o(q_0)$$</p>

### 9.4 Baum-Welch Algorithm

### 9.5 Hidden Markov Code
ì‹¤ì œ Packageë¡œì„œ hmm learn(https://hmmlearn.readthedocs.io/en/latest/)ë¥¼ ì œê³µí•˜ë‚˜ ì‹œë„í•´ë³´ê³ ì í•˜ëŠ” Datasetì´ ì ì–´ì„œ ì˜ ì‘ë™í•˜ì§€ ì•Šì•˜ë‹¤.  

ë”°ë¼ì„œ Low Levelì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” Implement Viterbi Algorithm in Hidden Markov Model using Python and R(http://www.adeveloperdiary.com/data-science/machine-learning/implement-viterbi-algorithm-in-hidden-markov-model-using-python-and-r/)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.


```python
import pandas as pd
import numpy as np

np.random.seed(30)

def forward(V, a, b, initial_distribution):
    alpha = np.zeros((V.shape[0], a.shape[0]))
    alpha[0, :] = initial_distribution * b[:, V[0]]
 
    for t in range(1, V.shape[0]):
        for j in range(a.shape[0]):
            # Matrix Computation Steps
            #                  ((1x2) . (1x2))      *     (1)
            #                        (1)            *     (1)
            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]
 
    return alpha
 

def backward(V, a, b):
    beta = np.zeros((V.shape[0], a.shape[0]))
 
    # setting beta(T) = 1
    beta[V.shape[0] - 1] = np.ones((a.shape[0]))
 
    # Loop in backward way from T-1 to
    # Due to python indexing the actual loop will be T-2 to 0
    for t in range(V.shape[0] - 2, -1, -1):
        for j in range(a.shape[0]):
            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])
 
    return beta
 

def baum_welch(V, a, b, initial_distribution, n_iter=100):
    M = a.shape[0]
    T = len(V)
 
    for n in range(n_iter):
        alpha = forward(V, a, b, initial_distribution)
        beta = backward(V, a, b)
 
        xi = np.zeros((M, M, T - 1))
        for t in range(T - 1):
            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])
            for i in range(M):
                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T
                xi[i, :, t] = numerator / denominator
 
        gamma = np.sum(xi, axis=1)
        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))
 
        # Add additional T'th element in gamma
        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))
 
        K = b.shape[1]
        denominator = np.sum(gamma, axis=1)
        for l in range(K):
            b[:, l] = np.sum(gamma[:, V == l], axis=1)
 
        b = np.divide(b, denominator.reshape((-1, 1)))
 
    return (a, b)
 

def viterbi(V, a, b, initial_distribution):
    T = V.shape[0]
    M = a.shape[0]
 
    omega = np.zeros((T, M))
    omega[0, :] = np.log(initial_distribution * b[:, V[0]])
 
    prev = np.zeros((T - 1, M))
 
    for t in range(1, T):
        for j in range(M):
            # Same as Forward Probability
            probability = omega[t - 1] + np.log(a[:, j]) + np.log(b[j, V[t]])
 
            # This is our most probable state given previous state at time t (1)
            prev[t - 1, j] = np.argmax(probability)
 
            # This is the probability of the most probable state (2)
            omega[t, j] = np.max(probability)
 
    # Path Array
    S = np.zeros(T)
 
    # Find the most probable last hidden state
    last_state = np.argmax(omega[T - 1, :])
 
    S[0] = last_state
 
    backtrack_index = 1
    for i in range(T - 2, -1, -1):
        S[backtrack_index] = prev[i, int(last_state)]
        last_state = prev[i, int(last_state)]
        backtrack_index += 1
 
    # Flip the path array since we were backtracking
    S = np.flip(S, axis=0)
 
    # Convert numeric values to actual hidden states
    result = []
    for s in S:
        if s == 0:
            result.append("q0")
        elif s==1:
            result.append("q1")
        elif s==2:
            result.append("q2")
 
    return result
 

    
data = pd.read_csv('./data.csv')
 
V = data['Visible'].values
 
# Transition Probabilities
a = np.ones((3, 3))
a = a / np.sum(a, axis=1)
 
# Emission Probabilities
b = np.ones((3,5))
b = b / np.sum(b, axis=1).reshape((-1, 1))
 
# Equal Probabilities for the initial distribution
initial_distribution = np.array((1.0, 0.0, 0.0))
 
transition, emission = baum_welch(V, a, b, initial_distribution, n_iter=100)
print('Transition')
print(transition)
print()

print('Emssion')
emission = emission / np.sum(emission, axis=1).reshape((-1, 1))
print(emission)
print()

pred = viterbi(V, transition, emission , initial_distribution)

count = 0  
TP = 0  
FP = 0  
   
for i,p in enumerate(pred):
    if p == 'q1':
        FP+=1
        if p == data['Hidden'][i]:
            FP-=1
            TP+=1
    if p == data['Hidden'][i]:
        count+=1
        
print('Accuracy',count/len(data))
print('Precision', TP/(TP+FP))  
   
print(pred)  
```

    Transition
    [[0.         0.5        0.5       ]
     [0.02941173 0.48529414 0.48529414]
     [0.02941173 0.48529414 0.48529414]]
    
    Emssion
    [[1.28376738e-27 2.60099443e-23 1.86212486e-23 3.11269808e-28
      1.00000000e+00]
     [2.64705873e-01 2.94117637e-01 2.05882346e-01 2.35294109e-01
      3.51426358e-08]
     [2.64705873e-01 2.94117637e-01 2.05882346e-01 2.35294109e-01
      3.51426358e-08]]
    
    Accuracy 0.6388888888888888
    Precision 0.6176470588235294
    ['q0', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q0']


    /root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log
    /root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: divide by zero encountered in log


ìœ„ì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê²Œ ë˜ë©´ AccuracyëŠ” 64%ì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆìœ¼ë‚˜, ì²˜ìŒê³¼ ë§ˆì§€ë§‰ì„ q0ë¼ê³  íŒë‹¨í•˜ê³ , ê·¸ ì™¸ì—ëŠ” q1ìœ¼ë¡œ íŒë‹¨í•˜ê²Œ ëœë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” Initializationì„ ì˜ëª» í•˜ì˜€ë‹¤ê³  ì¶”ì¸¡í•˜ì˜€ë‹¤. ì¦‰, AccuracyëŠ” ë†’ì•„ë„ Modelì˜ Precisionì€ ë§ì´ ë¶€ì¡±í•œ ìƒí™©ì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

**HMMì€ E-M Algorithmì´ë‹¤. ì¦‰, Local Minima or Maximaì— ë¹ ì§ˆ ìˆ˜ ìˆëŠ” ìƒí™©ì´ ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ê²ƒì„ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ ê¸°ì¡´ì— ê°€ì§€ê³  ìˆëŠ” Dataë¥¼ ê°€ì§€ê³  Initializationê°’ì„ ê´€ì¸¡ëœ Dataì˜ MLEê°’ìœ¼ë¡œì„œ ë³€ê²½í•˜ì—¬ ê°’ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.**

**MLE of Emission Probability**


```python
# Calculate Emission Probability  
q0 = data[data["Hidden"]=="q0"]  
q1 = data[data["Hidden"]=="q1"]  
q2 = data[data["Hidden"]=="q2"]  

q0_mle_list = []
q1_mle_list = []
q2_mle_list = []  


for i in range(5):
    q0_mle_list.append(len(q0[q0['Visible']==i])/len(q0))  

    
for i in range(5):
    q1_mle_list.append(len(q1[q1['Visible']==i])/len(q1))  

    
for i in range(5):
    q2_mle_list.append(len(q2[q2['Visible']==i])/len(q2))  

    
print('q0 MLE Probability')
print(q0_mle_list)
print()  


print('q1 MLE Probability')
print(q1_mle_list)
print()  

print('q2 MLE Probability')
print(q2_mle_list)  
emission_initial = np.stack((q0_mle_list,q1_mle_list,q2_mle_list),axis=0)  
```

    q0 MLE Probability
    [0.0, 0.0, 0.0, 0.0, 1.0]
    
    q1 MLE Probability
    [0.2857142857142857, 0.23809523809523808, 0.19047619047619047, 0.2857142857142857, 0.0]
    
    q2 MLE Probability
    [0.23076923076923078, 0.38461538461538464, 0.23076923076923078, 0.15384615384615385, 0.0]


**MLE of Transimission Probability**


```python
# Calculate Transimission Probability  
transmission_array = np.array(((0,0,0),(0,0,0),(0,0,0)))  
d = ["q0","q1","q2"]  

for i in range(len(data)-1):
    before = data["Hidden"][i]
    after = data["Hidden"][i+1]  
    
    for i,value in enumerate(d):
        for j,value2 in enumerate(d):
            if before == value and after == value2:
                transmission_array[i,j]+=1  

                
transmission_initial = transmission_array / np.sum(transmission_array, axis=1).reshape((-1, 1))  
print('Transmission of Probability')  
print(transmission_initial)  
```

    Transmission of Probability
    [[0.         1.         0.        ]
     [0.04761905 0.85714286 0.0952381 ]
     [0.         0.15384615 0.84615385]]


**Implement Viterbi Algorithm in Hidden Markov Model using Python with MLE Initial Probability**


```python
transition, emission = baum_welch(V, transmission_initial, emission_initial, initial_distribution, n_iter=100)
print('Transition')
print(transition)
print()

print('Emssion')
emission = emission / np.sum(emission, axis=1).reshape((-1, 1))
print(emission)
print()

pred = viterbi(V, transition, emission , initial_distribution) 

count = 0  
TP = 0  
FP = 0  
   
for i,p in enumerate(pred):
    if p == 'q1':
        FP+=1
        if p == data['Hidden'][i]:
            FP-=1
            TP+=1
    if p == data['Hidden'][i]:
        count+=1
        
print('Accuracy',count/len(data))
print('Precision', TP/(TP+FP))  
   
print(pred)
```

    Transition
    [[0.         1.         0.        ]
     [0.06640658 0.32785813 0.60573529]
     [0.         0.48157408 0.51842592]]
    
    Emssion
    [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
      1.00000000e+00]
     [5.63565953e-01 4.81671036e-05 7.71107067e-05 4.36308770e-01
      0.00000000e+00]
     [2.71049580e-02 5.27909937e-01 3.69502457e-01 7.54826478e-02
      0.00000000e+00]]
    
    Accuracy 0.6111111111111112
    Precision 0.7058823529411765
    ['q0', 'q1', 'q1', 'q2', 'q2', 'q1', 'q1', 'q1', 'q1', 'q2', 'q2', 'q1', 'q1', 'q2', 'q2', 'q1', 'q2', 'q1', 'q2', 'q1', 'q1', 'q2', 'q2', 'q1', 'q2', 'q2', 'q2', 'q1', 'q2', 'q2', 'q1', 'q1', 'q2', 'q2', 'q1', 'q0']


    /root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log
    /root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: divide by zero encountered in log


ìœ„ì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê²Œ ë˜ë©´, Accuracy(64% -> 61%)ëŠ” ë–¨ì–´ì¡Œìœ¼ë‚˜, í›¨ì”¬ ë” Precision(62% -> 70%)ì´ ë†’ì•„ì§„ ìƒí™©ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
ìƒí™©ì— ë”°ë¼ì„œ, ë” ì¢‹ì€ Initializationì„ ì„ íƒí•˜ë©´ ë  ê²ƒì´ë‹¤.

**Datasetì˜ ì ê³  Precisionì˜ ì¤‘ìš”ë„ì— ë”°ë¼ì„œ ìœ„ì˜ Modelì—ì„œ Initializationì„ ì–´ë–»ê²Œ í• ì§€ ì •í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë°©ë²•ì´ë¼ê³  ìƒê°ëœë‹¤.**
