---
layout: post
title:  "Paper29. Auto-Encoding Variational Bayes"
date:   2022-08-28 10:00:20 +0700
categories: [Paper]
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

###  Auto-Encoding Variational Bayes
이 Poster는 아래 논문과 Blog들을 참조하여 정리하였다는 것을 먼저 밝힙니다.

- Paper: https://arxiv.org/pdf/1312.6114.pdf
- 참조 Blog
    - 1. <a href="https://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-1.html">초짜 대학원생의 입장에서 이해하는 Auto-Encoding Variational Bayes (VAE) (1)</a>,<a href="https://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-2.html">(2)</a>, <a href="https://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-3.html">(3)</a>
    - 2. <a href="https://greeksharifa.github.io/generative%20model/2019/03/03/GAN/">Gorio Learning Blog - VAE</a>
    - 3. <a href="https://ratsgo.github.io/generative%20model/2017/12/19/vi/">Ratsgo's Blog - Variational Inference</a>
    - 4. <a href="https://hugrypiggykim.com/2018/09/07/variational-autoencoder%EC%99%80-elboevidence-lower-bound/">TENSORMSA Blog</a>
    - 5. <a href="https://andacdemir-94699.medium.com/auto-encoding-variational-bayes-c0a2e0743aac">Andac Demir Blog</a>

### Abstract
>How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? 
We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. 
First, **we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.** 
Second, **we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.** 
Theoretical advantages are reflected in experimental results.

VAE (Variational AutoEncoder)의 목적은 random variable에서 원본 input을 어떻게 generation 할 것인지에 대한 model이며, 이에 대하여 생기는 문제에 대하여 approximation하여 해결할 수 있는 방안을 제시한다.

### Notation
- <span>$$X = \{x^{(i)}\}_{i=1}^N$$</span>: i.i.d. samples. 
- <span>$$z^{(i)}$$</span>: Unobserved continuous random variable.
- <span>$$\theta$$</span>: Parameter of Generator (Decoder)
- <span>$$\phi$$</span>: Parameter of Discriminator (Encoder)
- <span>$$p_{\theta^*} (z)$$</span>: Prior distribution. <span>$$z^{(i)}$$</span> generated from <span>$$p_{\theta^*} (z)$$</span>
- <span>$$p_{\theta^*} (x|z)$$</span>: Conditional distribution. <span>$$x^{(i)}$$</span> generated from <span>$$p_{\theta^*} (x|z)$$</span>
- <span>$$q_{\phi} (z|x)$$</span>: Encoder
- <span>$$p_{\theta} (x|z)$$</span>: Decoder

### Problem
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Paper/VAE/2.png)

Generation Model인 VAE에서 해결하고자 하는 문제는 위의 Figure와 같다.  
즉, **Data (<span>$$x$$</span>)는 어떠한 random variable에서 <span>$$z$$</span> 생성 (<span>$$p_{\theta^*} (x|z)$$</span>)되었다고 가정하게 되면, <span>$$z$$</span>와 <span>$$\theta$$</span>를 알 수 있다면, sampling을 통하여 생성할 수 있다는 것 이다.**

하지만, 위와 같은 문제를 해결하기 위해서는 해결해야하는 2가지 문제점이 있다.
1. Random variable (<span>$$z$$</span>)는 측정할 수 없는 값 이라는 것 이다. 이러한 문제점을 해결하기 위하여 해당 논문에서는 Encoder를 활용하여 Data (<span>$$x$$</span>)를 통하여<span>$$z$$</span>를 예측한다.
2. <span>$$\theta$$</span>를 모른다는 점이다. 이것은 Model구성 시에 충분히 Training을 통하여 학습될 수 있는 값 이다.

### Variance Inference

VAE의 Loss Function을 이해하기 위하여 먼저 알고 있어야 되는 개념이 "Variance Inference (VI)"이다.  
"VI란 사후확률(posterior)분포 <span>$$p(z|x)$$</span>를 다루기 쉬운 확률분포 <span>$$q(z)$$</span>로 approximation하는 것을 의미한다."  
사용하는 경우는 다음과 같이 3가지 경우가 있다.
- marginal probability, 즉 posterior의 분모인 <span>$$p(x) = \sum_{z}p(x,z)$$</span>를 계산하기 어려운 경우
- likelihood, 즉<span>$$p(x|z)$$</span>를 더 복잡하게 모델링 하고 싶은 경우
- prior, 즉 <span>$$p(z)$$</span>를 더 복잡하게 모델링 하고 싶은 경우

**KL Divergence**  
어떠한 두 분포 사이에서 유사도를 계산하기 위한 방법으로서는 <a href="https://wjddyd66.github.io/pytorch/Pytorch-GAN/">Kullback-Leibler divergence(KL-Divergence)</a>을 사용하게 된다.  
즉, <span>$$p(z|x)$$</span>와 <span>$$q(z)$$</span>와 사이의 KLD를 최소화할 수 있는 <span>$$q^{*}(z)$$</span>를 구한다.


<p>$$D_{KL}(q(z) || p(z|x)) = \int q(z) \log \frac{q(z)}{p(z|x)} dz$$</p>
<p>$$= \int q(z) \log \frac{q(z) p(x)}{p(x|z) p(z)} dz$$</p>
<p>$$(\because p(x)p(z|x) = p(x|z)p(z))$$</p>
<p>$$= \int q(z) \log \frac{q(z)}{p(z)} dz + \int q(z) \log p(x) dz - \int q(z) \log p(x|z) dz$$</p>
<p>$$= D_{KL}(q(z) || p(z)) + \log p(x) - E_{z \text{~} q(z)}[ \log p(x|z) ]$$</p>

**Variational Inference with Monte Carlo sampling**  
몬테카를로 방법(Monte Carlo Method)이란 랜덤 표본을 뽑아 함수의 값을 확률적으로 계산하는 알고리즘이다.  
예를들어 특정확률 분포를 따르는 x의함수값의 기대값은 다음과 같이 k개의 sample로서 근사하는 식은 아래와 같다.  
<p>$$\int { p\left( x \right) f\left( x \right) dx } ={ E }_{ x\sim p\left( x \right)  }\left[ f(x) \right] \approx \frac { 1 }{ K } \sum _{ i=0 }^{ K }{ { \left[ f({ x }_{ i }) \right]  }_{ { x }_{ i }\sim p\left( x \right)  } }$$</p>

위와 같은 Monte Carlo Method을 KLD에 적용하면 식을 다음과 같이 정리할 수 있다.

<p>$$D_{KL}(q(z) || p(z|x)) = D_{KL}(q(z) || p(z)) + \log p(x) - E_{z \text{~} q(z)}[ \log p(x|z) ]$$</p>
<p>$$E_{z \sim q(z)} [\log \frac{q(z)}{p(z)}] + \log p(x) - E_{z \sim q(z)} [\log p(x|z)]$$</p>
<p>$$\approx \frac{1}{K} \sum_{i=0}^K [\log \frac{q(z_i)}{p(z_i)}]_{z_i \sim q(z)} + \log p(x) - \frac{1}{K} \sum_{i=0}^K [\log p(x|z_i)]_{z_i \sim q(z)}$$</p>
<p>$$= \frac{1}{K} \sum_{i=0}^K [\log q(z_i) - \log p(z_i) - \log p(x|z_i)]_{z_i \sim q(z)}+\log p(x)$$</p>

위와 같이 Monte Carlo Method로서 식을 해결하면, <span>$$q(z) (\approx p(z|x))$$</span>를 원하는 Distribution으로서 정할 수 있다. (E.X.) 동전던지기에서는 Posterior가 Beta-Distribution일 것 이다.)  
하지만, 실제 해결해야하는 문제에서는 Posterior를 구하기 어렵지만, 어떠한 Distribution으로서도 나타낼 수 있게 된다.

**Variational Inference with SGD**  
VI를 해결하는 방법으로서 Stochastic Variational Inference (SVI)를 사용하는 방법입니다.
위의식을 미분하기 위하여 먼저, 각각의 Distribution을 다음과 같다고 지정하고 식을 전개하자.

- <span>$$q(z), (\theta_{q} = \mu_q, \sigma_q)$$</span>: Normal Distribution
- <span>$$p(z), (\alpha, \beta)$$</span>: Beta Distribution


<p>$$\frac{\partial}{\partial \theta_q} D_{KL} (q(z) || p(z|x)) = \frac{\partial}{\partial \theta_q} D_{KL} (q(z) || p(z)) + \frac{\partial}{\partial \theta_q} \log p(x) - \frac{\partial}{\partial \theta_q} E_{z \sim q(z)} [\log p(x|z)]$$</p>
<p>$$= \frac{\partial}{\partial \theta_q} E_{z \sim q(z)} [\log q(z) - \log p(z) - \log p(x|z)]$$</p>

위의 수식을 살펴보게 되면, <span>$$\partial / \partial \theta_{q}$$</span>가 Exception안으로 들어가야 하는데 z자체가 <span>$$\theta_{q}$$</span>에 의존하는 Distribution이므로 미분이 불가능한 것을 알 수 있다.

따라서, z대신에 noise(<span>$$\epsilon$$</span>)을 사용하여 z를 나타낸다.

<p>$$z = \mu_q + \sigma_q \epsilon, \epsilon \sim N(0,1)$$</p>
<p>$$\frac{\partial}{\partial \theta_q} D_{KL} (q(z) || p(z|x))$$</p>
<p>$$= \frac{\partial}{\partial \theta_q} E_{\epsilon \sim N(0,1)} [\log q (\mu_q + \sigma_q \epsilon) - \log p(\mu_q + \sigma_q \epsilon) - \log p(x|z = \mu_q + \sigma_q \epsilon)]$$</p>
<p>$$= E_{\epsilon \sim N(0,1)} [\frac{\partial}{\partial \theta_q} \{ \log q(\mu_q + \sigma_q \epsilon) - \log p (\mu_q + \sigma_q \epsilon) - \log p(x|z = \mu_q + \sigma_q \epsilon) \}]$$</p>
<p>$$= \frac{1}{K} \sum_{i=0}^K [\log q(\mu_q + \sigma_q \epsilon) - \log p (\mu_q + \sigma_q \epsilon) - \log p(x|z = \mu_q + \sigma_q \epsilon)]_{\epsilon \sim N(0,1)} $$</p>
<p>$$(\because \text{Using Monte Carlo Method})$$</p>

**Vatiational EM algorithm**  
VI와 SVI모두, <span>$$q(z)$$</span>의 parameter를 학습하는 방식이나, 이는 모두 <span>$$p(z) (\text{prior}), p(x|z) (\text{likelihood})$$</span>를 모두 알 고 있는 경우에 값을 구할 수 있다.

하지만, 실제 Likelihood와 Prior를 모르는 경우도 많이 있습니다. 이를 해결하기 위하여 다음과 같이 Parameter를 설정하고 EM algorithm을 통하여 식을 전개합니다. (<span>$$p(z)$$</span>의 값은 임의로 고정시켜고 지장이 없습니다.)

- <span>$$\theta_{q}$$</span>: Parameter of <span>$$q(z)$$</span>
- <span>$$\theta_{l}$$</span>: Parameter of <span>$$p(x|z)$$</span>

위의 두 Parameter를 Update하기 위하여 아래와 같은 EM-Algorithm을 사용하게 된다.
- Expectation: <span>$$D_{KL} (q(z) || p(z|x))$$</span>를 줄이는 <span>$$\theta_q$$</span>를 찾는다. (VI or SVI)
- Maximization: E-step에서 찾은 <span>$$\theta_q$$</span>를 고정한 상태에서 <span>$$\log p(x)$$</span>의 lower bound를 최대화 하는 <span>$$p(x|z)$$</span>의 <span>$$\theta_{l}$$</span>을 update한다.

먼저 Exception과정의 최종적인 식은 아래와 같다.
<p>$${ E }_{ x\sim p\left( x \right)  }\left[ f(x) \right] \approx \frac { 1 }{ K } \sum _{ i=0 }^{ K }{ { \left[ f({ x }_{ i }) \right]  }_{ { x }_{ i }\sim p\left( x \right)  } }$$</p>
<span>$$p(x)$$</span>에 대하여 식을 정리하면 아래와 같다.
<p>$$\log p(x) = E_{z \sim p(z)}[\log p(x|z)] - D_{KL} (q(z) || p(z)) + D_{KL} ( q(z) || p(z|x))$$</p>
위의 식에서 <span>$$D_{KL} ( q(z) || p(z|x)$$</span>은 항상 양수의 값이므로, update해야하는 <span>$$p(x|z)$$</span>에 대하여 식을 다시 정리하면 아래와 같다. 
<p>$$\log p(x) \ge E_{z \sim q(z)} [\log p(x|z)] - D_{KL} (q(z) || p(z))$$</p>

즉, **Evidence <span>$$p(x)$$</span>를 Maximization하는 <span>$$\theta_l$$</span>을 찾는 과정으로 해당 식은 Evidence Lower Bound(ELBO)라고 불리게 된다.**

### The variational bound
위의 <a href="">Variance Inference</a>의 식을 이해하게 되면, 이제 VAE의 Loss Function을 이해할 수 있다.  
GAN의 Loss Function은 아래와 같다.

<p>$$\log p_{\theta}(x^{(1)}, \ldots, x^{(N)}) = \sum_{i=1}^N \log p_{\theta} (x^{(i)}) , (\because \text{Monte Carlo sampling})$$</p>
<p>$$\log p_{\theta} (x^{(i)}) = D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta} (z|x^{(i)})) + L(\theta, \phi, x^{(i)})$$</p>
<p>$$\log p_{\theta}(x^{(i)}) \ge L(\theta, \phi, x^{(i)}) = E_{q_{\phi}(z|x)} [-\log q_{\phi}(z|x^{(i)}) + \log p_{\theta}(x^{(i)}|z)]$$</p>
<p>$$ L(\theta, \phi, x^{(i)}) = E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)] - D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z)), (\because \text{Vatiational EM algorithm})$$</p>


위에 대한 식을 수식적으로 이해하는데에는 어렵지 않다.  
이러한 위의 식에서 각각의 수식의 Term은 아래와 같이 이해하면 이해하기가 쉽다.

<img src="https://i0.wp.com/hugrypiggykim.com/wp-content/uploads/2018/09/VAE_ELBO_kimsu.014.jpeg?resize=750%2C563"><br>  
<img src="https://i0.wp.com/hugrypiggykim.com/wp-content/uploads/2018/09/VAE_ELBO_kimsu.015.jpeg?resize=750%2C563"><br>  
<img src="https://i0.wp.com/hugrypiggykim.com/wp-content/uploads/2018/09/VAE_ELBO_kimsu.016.jpeg?resize=750%2C563"><br>  

그림 출처: <a href="https://hugrypiggykim.com/2018/09/07/variational-autoencoder%EC%99%80-elboevidence-lower-bound/">hugrypiggykim Blog</a>

위의 식에서 문제가 되는 Term은 <span>$$E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)]$$</span>이다.  
<span>$$E_{q_{\phi}(z|x^(i) )} (\cdot)$$</span>을 살펴보게 되면, <span>$$q(z|x)$$</span>에서 여러개를 sampling하면 되나, backward에서는 sampling에대한 Gradient를 전파하지 못한다는 문제가 발생하게 된다.  

이러한 문제 해결방법을 위하여 해당 논문에서는 "reparameterization trick"을 사용하였다.

### Solution <span>$$-D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z)))$$</span>, Gaussian Case

위에서 구한 수식은 다시한번 살펴보면 아래와 같다.  
<p>$$ L(\theta, \phi, x^{(i)}) = - D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z))) + E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)] $$</p>

위의 수식은 <span>$$D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z)))$$</span> (1-st Term: Regularization)과 <span>$$E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)]$$</span> (2-nd Term: Reconstruction Error)으로서 나눌 수 있다.  

위에 수식에서 문제가 되는 것은 Sampling을 수행해야하는 2-nd Term이고 1-st Term은 아래와 같이 풀어서 사용할 수 있다.

- <span>$$p_{\theta}(z) = N(0,I)$$</span>: Prior
- <span>$$q_{\phi}(z|x^{(i)})$$</span>: Posterior approximation (Gaussian)
- <span>$$u_i, \sigma_i$$</span>: i-th element mean and variance of posterior approximation
- <span>$$u_j, \sigma_j$$</span>: j-th element mean and variance of posterior approximation
- <span>$$J$$</span>: Dimension of <span>$$z$$</span>

또한 위의 식을 쉽게 풀기 위하여 <span>$$q_{\phi}(z|x^{(i)}), p_{\theta}(z)$$</span>를 <span>$$q(x), p(x)$$</span>로서 표현하고 각각의 평균과 분산을 <span>$$\mu_1, \sigma_1, \mu_2, \sigma_2$$</span>이고, univariate Gaussian이라 생각하면 아래와 같이 식을 쓸 수 있다.

<p>$$D_{KL} (q (x) || p(x))$$</p>

<p>$$=\int q(x) (\log q(x) - \log p(x)) dx$$</p>

<p>$$=\int [-\frac{1}{2} \log (2\pi) - \log (\sigma_1) - \frac{1}{2}(\frac{(x-\mu_1)}{\sigma_1})^2 + \frac{1}{2} \log (2\pi) + \log (\sigma_2) + \frac{1}{2}(\frac{(x-\mu_2)}{\sigma_2})^2] \times \frac{1}{\sqrt{2\pi}\sigma_1} \text{exp} [-\frac{1}{2} (\frac{x - \mu_1}{\sigma_1})^2]  dx$$</p>

<p>$$= \int \{ \log(\frac{\sigma_2}{\sigma_1}) + \frac{1}{2} [(\frac{x - \mu_2}{\sigma_2})^2 - (\frac{x - \mu_1}{\sigma_1})^2] \} \times \frac{1}{\sqrt{2\pi}\sigma_1} \text{exp} [-\frac{1}{2} (\frac{x - \mu_1}{\sigma_1})^2] dx$$</p>

<p>$$E \{ \log(\frac{\sigma_2}{\sigma_1}) + \frac{1}{2} [(\frac{x - \mu_2}{\sigma_2})^2 - (\frac{x - \mu_1}{\sigma_1})^2] \} (\because \int \frac{1}{\sqrt{2\pi}\sigma_1} \text{exp} [- \frac {1}{2} (\frac{x - \mu_1}{\sigma_1})^2] dx = 1)$$</p>

<p>$$= \log (\frac{\sigma_2}{\sigma_1}) + \frac{1}{2\sigma_2^2} E \{ (X-\mu_2)^2 \} - \frac{1}{2\sigma_1^2} E \{ (X-\mu_1)^2 \}$$</p>

<p>$$= \log (\frac{\sigma_2}{\sigma_1}) + \frac{1}{2\sigma_2^2} E \{ (X-\mu_2)^2 \} - \frac{1}{2} (\because E \{ (X-\mu_1)^2 \} = \sigma_1^2)$$</p>

This expectation term can be reformulated,
<p>$$(X - \mu_2)^2 = (X - \mu_1 + \mu_1 - \mu_2)^2 = (X-\mu_1)^2 + 2(X-\mu_1)(\mu_1 - \mu_2) + (\mu_1 - \mu_2)^2$$</p>

Hence,

<p>$$= \log(\frac{\sigma_2}{\sigma_1}) + \frac{1}{\sigma_2^2} [E\{ (X-\mu_1)^2 \} + 2(\mu_1 - \mu_2) E\{ (X-\mu_1) \} + (\mu_1-\mu_2)^2]-\frac{1}{2}$$</p>

<p>$$= \log (\frac{\sigma_2}{\sigma_1}) + \frac{\sigma_1^2 (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$</p>


<p>$$ -\log (\sigma_1) + \frac{\sigma_1^2 + \mu_1^2}{2} -\frac{1}{2}(\because, \mu_2=0, \sigma_2=1)$$</p>



최종적인 위의 식을 J dimension의 multivariate Gaussian이라 생각하면 최종적인 식은 다음과 같이 적을 수 있다.
<p>$$-D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z))) = \frac{1}{2} \sum_{j=1}^J (1 + \log ((\sigma_j)^2)-(\mu_j)^2 - (\sigma_j)^2)$$</p>


###  The reparameterization trick
최종적인 Loss Function을 생각해보면 아래와 같다.  
<p>$$ L(\theta, \phi, x^{(i)}) = - D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z))) + E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)] $$</p>

위에 수식에서 문제가 되는 것은 Sampling을 수행해야하는 2-nd Term은 <span>$$g_{\phi}(\epsilon, x)$$</span>라는 미분 가능한 함수와 보조 noise variable <span>$$\epsilon$$</span>으로 나타낼 수 있다.

<p>$$\tilde{z} = g_{\phi}(\epsilon, x) \text{ with } \epsilon \sim p(\epsilon)$$</p>
<p>$$E_{q_{\phi}(z|x^(i))} [f(z)] = E_{p(\epsilon)} [ f(g_{\phi}(\epsilon, x^{(i)})) ] = \frac{1}{L} \sum_{l=1}^{L} f(g_{\phi}(\epsilon^{(l)}, x^{(i)})) \text{, where } \epsilon^{(l)} \sim p(\epsilon)
$$</p>
<p>$$(\because \text{Monte Carlo sampling})$$</p>

위에서 <span>$$q_{\phi}(z|x^{(i)})$$</span>는 Gaussian Distribution이였습니다. 이것을 다시 나타내면 <span>$$Q(Z|X) = N(\mu(X), \sum(X))$$</span>이고 이를 <span>$$\epsilon \sim N(0,I)$$</span>로서 sampling한 형태로 나타내게 되면, <span>$$z = \mu(X) + \sum^{(1/2)} * \epsilon$$</span>로 변환되는 것을 알 수 있습니다.  

즉, 문제가 되었던 stochastic인 부분인 <span>$$q_{\phi} (z|x)$$</span>을 backprogagation과 상관없는 <span>$$\epsilon \sim p(\epsilon)$$</span>으로서 나타내어, deterministic한 <span>$$g_{\phi}(\epsilon, x)$$</span>으로 표현하여 <span>$$z = \mu(X) + \sum^{(1/2)} * \epsilon$$</span>값으로서 표현할 수 있다.  

이를 그림으로 보면 아래와 같다.

<img src="https://1.bp.blogspot.com/-V-m6dOVaUL8/WQ2JKJ4Jj4I/AAAAAAAABrA/BjxqKMDfR6ggYCCqUNlBFiS4cqlyisgKACK4B/s1600/vae_3.PNG"><br>
그림 출처: <a href="https://jaejunyoo.blogspot.com/2017/05/auto-encoding-variational-bayes-vae-3.html">초짜 대학원생의 입장에서 이해하는 Auto-Encoding Variational Bayes (VAE) (3)</a>

각각의 정리한 식을 모두 적용하면 최종적인 식은 다음과 같다.
<p>$$ L(\theta, \phi, x^{(i)}) = - D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z))) + E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)] $$</p>
<p>$$= \frac{1}{2} \sum_{j=1}^J (1 + \log ((\sigma_j)^2)-(\mu_j)^2 - (\sigma_j)^2) + \frac{1}{L} \sum_{l=1}^{L} f(g_{\phi}(\epsilon^{(l)}, x^{(i)}))$$</p>
<p>$$\text{, where } z^{(i,l)} = g_{\phi}(\epsilon^{(i,l)}, x^{(i)}) \text{ and } \epsilon^{(l)} \sim p(\epsilon) \text{ and } J = \text{Dimension of }z$$</p>

**Appendix 1. <span>$$\log p_{\theta}(x^{(i)}|z)$$</span>**  

<span>$$\log p_{\theta}(x^{(i)}|z)$$</span>을 실제 예제에 대입하여 저자들은 아래와 같이 Loss Function을 예시를 들었다.

![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Paper/VAE/4.png)

**Appendix 2. <span>$$q_{\phi} (z|x), g_{\phi}(\cdot), \epsilon \sim p(\epsilon)$$</span>**  

위의 수식을 이해할때에는 모두 Gaussian Distribution으로서 가정하고 식을 전개하고 알아보았다.  
각각에 대하여 어떤 Distribution으로 두어야되는지에 대해서 저자는 다음과 같이 설명하고 있다.

![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/Paper/VAE/3.png)

### Pytorch Code
-Code 참조: <a href="https://cumulu-s.tistory.com/25">cumulu-s blog</a>

**Model**  
- <code>x</code>: samples (<span>$$X = \{x^{(i)}\}_{i=1}^N$$</span>)
- <code>encode(self, x)</code>: Encoder (<span>$$q_{\phi} (z|x)$$</span>)
- <code>decode(self, z)</code>: Decoder (<span>$$p_{\theta} (x|z)$$</span>)
- <code>self.fc31(h2)</code>: mean of z (<span>$$\mu$$</span>)
- <code>self.fc32(h2)</code>: log variance of z (<span>$$\log (\sigma)$$</span>)
- <code>eps = torch.randn_like(std)</code>: <span>$$\epsilon \sim N(0, I)$$</span>
- <code>reparameterize(self, mu, logvar)</code>: <span>$$z = \mu(X) + \sum^{(1/2)} * \epsilon$$</span>


```python
# VAE model
class VAE(nn.Module):
    def __init__(self, image_size, hidden_size_1, hidden_size_2, latent_size):
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(image_size, hidden_size_1)
        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)
        self.fc31 = nn.Linear(hidden_size_2, latent_size)
        self.fc32 = nn.Linear(hidden_size_2, latent_size)

        self.fc4 = nn.Linear(latent_size, hidden_size_2)
        self.fc5 = nn.Linear(hidden_size_2, hidden_size_1)
        self.fc6 = nn.Linear(hidden_size_1, image_size)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        return self.fc31(h2), self.fc32(h2)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decode(self, z):
        h3 = F.relu(self.fc4(z))
        h4 = F.relu(self.fc5(h3))
        return torch.sigmoid(self.fc6(h4))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```

**Loss Function**  

<p>$$ L(\theta, \phi, x^{(i)}) = - D_{KL} (q_{\phi} (z|x^{(i)}) || p_{\theta}(z))) + E_{q_{\phi}(z|x^(i))} [\log p_{\theta}(x^{(i)}|z)] $$</p>
<p>$$= \frac{1}{2} \sum_{j=1}^J (1 + \log ((\sigma_j)^2)-(\mu_j)^2 - (\sigma_j)^2) + \frac{1}{L} \sum_{l=1}^{L} f(g_{\phi}(\epsilon^{(l)}, x^{(i)}))$$</p>
<p>$$\text{, where } z^{(i,l)} = g_{\phi}(\epsilon^{(i,l)}, x^{(i)}) \text{ and } \epsilon^{(l)} \sim p(\epsilon) \text{ and } J = \text{Dimension of }z$$</p>

- <code>-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())</code>: <span>$$\frac{1}{2} \sum_{j=1}^J (1 + \log ((\sigma_j)^2)-(\mu_j)^2 - (\sigma_j)^2)$$</span>
- <code>F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction = 'sum')</code>: <span>$$\sum_{i=1}^D x_i \log y_i + (1-x_i) \cdot \log (1-y_i), (\because \log p(x|z) \text{ is Bernoulli})$$</span>


```python
def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction = 'sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE, KLD
```
